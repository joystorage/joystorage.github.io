[{"title":"STL unordered_set的使用","date":"2017-10-30T12:50:16.242Z","path":"2017/10/30/STL unordered_set的使用/","text":"12345template &lt; class Key, class Hash = hash&lt;Key&gt;, class Pred = equal_to&lt;Key&gt;, class Alloc = allocator&lt;Key&gt; &gt; class unordered_set; 1.使用前先定义(1)定义元素类1234567891011121314151617181920class Entry&#123;private: string key; Value value;public: Entry(const string str,Value * value)&#123; key = str; this-&gt;value.a = value-&gt;a; this-&gt;value.b = value-&gt;b; &#125; string getK() &#123; return key; &#125; Value getV() &#123; return value; &#125;&#125;; (2)定义比较函数1234567struct NodeEquality&#123; inline bool operator()(Entry* const&amp; _s1, Entry* const&amp; _s2) const &#123; return _s1-&gt;getV().a == _s2-&gt;getV().a &amp;&amp; _s1-&gt;getV().b == _s2-&gt;getV().b; &#125;&#125;; (3)定义HASH函数123456789101112namespace std&#123; template&lt;&gt; struct hash&lt;Entry*&gt; &#123; typedef Entry* argument_type; typedef std::size_t result_type; result_type operator()(argument_type const&amp; s) const &#123; return hash&lt;string&gt;()(s-&gt;getK()); &#125; &#125;;&#125; (4)定义unordered_set1unordered_set&lt;Entry *,std::hash&lt;Entry *&gt;, NodeEquality&gt; myset; 2.插入新元素1myset.insert(new Entry(&quot;123&quot;,new Value(2,4))); 说明 插入时首先会根据hash函数取hash，若有重复，则通过比较函数判断内容是否相同，若仍然相同，则不会插入，否则插入该项(由于key相同，新项会链表形式挂在上一项的后面) 3.查找1auto it = myset.find(new Entry(&quot;123&quot;,new Value(2,8))); 说明 查找时会同时根据hash函数以及比较函数判断，两者都相同时判断找到此项。 4.举个例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;iostream&gt;#include &lt;unordered_set&gt;using namespace std;class Value&#123;public: int a; int b; Value(int a, int b) &#123; this-&gt;a = a; this-&gt;b = b; &#125; Value() &#123; a = b = 0; &#125;&#125;;class Entry&#123;private: string key; Value value;public: Entry(const string str,Value * value)&#123; key = str; this-&gt;value.a = value-&gt;a; this-&gt;value.b = value-&gt;b; &#125; string getK() &#123; return key; &#125; Value getV() &#123; return value; &#125;&#125;;struct NodeEquality&#123; inline bool operator()(Entry* const&amp; _s1, Entry* const&amp; _s2) const &#123; return _s1-&gt;getV().a == _s2-&gt;getV().a &amp;&amp; _s1-&gt;getV().b == _s2-&gt;getV().b; &#125;&#125;;namespace std&#123; template&lt;&gt; struct hash&lt;Entry*&gt; &#123; typedef Entry* argument_type; typedef std::size_t result_type; result_type operator()(argument_type const&amp; s) const &#123; return hash&lt;string&gt;()(s-&gt;getK()); &#125; &#125;;&#125;int main(int argc, char *argv[])&#123; unordered_set&lt;Entry *,std::hash&lt;Entry *&gt;, NodeEquality&gt; myset; myset.insert(new Entry(&quot;123&quot;,new Value(2,4))); myset.insert(new Entry(&quot;123&quot;,new Value(2,5))); myset.insert(new Entry(&quot;1235&quot;,new Value(6,7))); myset.insert(new Entry(&quot;1232&quot;,new Value(2,8))); for(auto it = myset.begin(); it != myset.end(); it++) &#123; cout &lt;&lt; &quot;Key:&quot; &lt;&lt;(*it)-&gt;getK() &lt;&lt; &quot; Value:(&quot; &lt;&lt;(*it)-&gt;getV().a &lt;&lt; &quot;,&quot; &lt;&lt;(*it)-&gt;getV().b &lt;&lt; &quot;)&quot; &lt;&lt; endl; &#125; auto it = myset.find(new Entry(&quot;123&quot;,new Value(2,4))); if(it != myset.end() ) cout &lt;&lt; (*it)-&gt;getV().a &lt;&lt; &quot; &quot; &lt;&lt;(*it)-&gt;getV().b &lt;&lt; endl; else cout &lt;&lt; &quot;NULL&quot; &lt;&lt; endl; system(&quot;pause&quot;); return 0;&#125;","tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":"unordered_set","slug":"unordered-set","permalink":"http://yoursite.com/tags/unordered-set/"},{"name":"STL","slug":"STL","permalink":"http://yoursite.com/tags/STL/"}]},{"title":"flashcache-5-filebench测试","date":"2017-08-09T11:46:33.447Z","path":"2017/08/09/flashcache-5-filebench测试/","text":"在ubuntu上安装filebench的最新版本会有一堆bug，很麻烦，因此用了别人改过的go_filebench，其实两个是一样的，只是改动了一下安装的配置。 下载后解压，然后输入下面命令即可安装123./configuremakemake install 具体运行如下更加详细的过程可以参见博客filebench介绍1234567user@host$ sudo su[sudo] password for user:root@host# echo 0 &gt; /proc/sys/kernel/randomize_va_spaceroot@host# go_filebench Filebench Version 1.4.912102: 0.000: Allocated 170MB of shared memoryfilebench&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142root@user# go_filebench Filebench Version 1.4.912324: 0.000: Allocated 170MB of shared memoryfilebench&gt; load fileserver12462: 2.869: FileServer Version 2.2 personality successfully loaded12462: 2.869: Usage: set $dir=&lt;dir&gt;12462: 2.869: set $meanfilesize=&lt;size&gt; defaults to 13107212462: 2.869: set $nfiles=&lt;value&gt; defaults to 1000012462: 2.869: set $nthreads=&lt;value&gt; defaults to 5012462: 2.869: set $meanappendsize=&lt;value&gt; defaults to 1638412462: 2.869: set $iosize=&lt;size&gt; defaults to 104857612462: 2.869: set $meandirwidth=&lt;size&gt; defaults to 2012462: 2.869: (sets mean dir width and dir depth is calculated as log (width, nfiles)12462: 2.869: run runtime (e.g. run 60)filebench&gt; set $dir=/mntfilebench&gt; run 6012462: 4.909: Creating/pre-allocating files and filesets12462: 4.918: Fileset bigfileset: 10000 files, avg dir width = 20, avg dir depth = 3.1, 1240.757MB12462: 5.280: Removed any existing fileset bigfileset in 1 seconds12462: 5.280: making tree for filset /tmp/bigfileset12462: 5.290: Creating fileset bigfileset...12462: 6.080: Preallocated 7979 of 10000 of fileset bigfileset in 1 seconds12462: 6.080: waiting for fileset pre-allocation to finish12466: 6.080: Starting 1 filereader instances12467: 6.081: Starting 50 filereaderthread threads12462: 7.137: Running...12462: 67.142: Run took 60 seconds...12462: 67.145: Per-Operation Breakdownstatfile1 128311ops 2138ops/s 0.0mb/s 0.0ms/op 2320us/op-cpu [0ms - 0ms]deletefile1 128316ops 2138ops/s 0.0mb/s 0.2ms/op 2535us/op-cpu [0ms - 458ms]closefile3 128323ops 2139ops/s 0.0mb/s 0.0ms/op 2328us/op-cpu [0ms - 0ms]readfile1 128327ops 2139ops/s 283.8mb/s 0.1ms/op 2460us/op-cpu [0ms - 267ms]openfile2 128329ops 2139ops/s 0.0mb/s 0.0ms/op 2332us/op-cpu [0ms - 2ms]closefile2 128332ops 2139ops/s 0.0mb/s 0.0ms/op 2332us/op-cpu [0ms - 0ms]appendfilerand1 128337ops 2139ops/s 16.6mb/s 0.1ms/op 2377us/op-cpu [0ms - 559ms]openfile1 128343ops 2139ops/s 0.0mb/s 0.0ms/op 2353us/op-cpu [0ms - 2ms]closefile1 128349ops 2139ops/s 0.0mb/s 0.0ms/op 2317us/op-cpu [0ms - 1ms]wrtfile1 128352ops 2139ops/s 265.2mb/s 0.1ms/op 2601us/op-cpu [0ms - 268ms]createfile1 128358ops 2139ops/s 0.0mb/s 0.1ms/op 2396us/op-cpu [0ms - 267ms]12462: 67.145: IO Summary: 1411677 ops, 23526 ops/s, (2139/4278 r/w), 565mb/s, 393us cpu/op, 0.2ms latency12462: 67.145: Shutting down processesroot@user#","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"},{"name":"filebench","slug":"filebench","permalink":"http://yoursite.com/tags/filebench/"}]},{"title":"flashcache-6-filebench测试","date":"2017-08-09T11:46:33.444Z","path":"2017/08/09/flashcache-6-利用blktrace记录IO，利用fio重放日志/","text":"详见博客黄金搭档之fio+blktrace-Linux下模拟块设备访问方式","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"},{"name":"blktrace","slug":"blktrace","permalink":"http://yoursite.com/tags/blktrace/"},{"name":"fio","slug":"fio","permalink":"http://yoursite.com/tags/fio/"}]},{"title":"C++读取配置文件","date":"2017-08-09T11:46:33.441Z","path":"2017/08/09/C++读取配置文件/","text":"一个读取配置文件的工具类，代码如下Config.h 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//////////////////////////////////////////////////////////////////////////// Config.h// Author: joyyzhang// Date: 2017-08#ifndef Config_h__#define Config_h__#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;using namespace std;#define LINE_LENGTH 500class Config &#123;public: Config() &#123; &#125; ~Config() &#123; &#125; void Get(const char *key_path, const char *para_name,char *result) &#123; ifstream fin(key_path); if(fin) &#123; char str[LINE_LENGTH]; while( fin.getline(str,LINE_LENGTH) ) &#123; if(str[0] == &apos;#&apos; || str[0] == &apos;\\0&apos;)//注释以#开头 continue; int cnt = 0; char tmp[2][LINE_LENGTH]; //以等号分割配置文件 const char *sep = &quot;=&quot;; //可按多个字符来分割 char *p; p = strtok(str, sep); strcpy(tmp[cnt],p); while(p)&#123; p = strtok(NULL, sep); cnt++; if(p) strcpy(tmp[cnt],p); else break; &#125; if(cnt == 2) &#123; if(strcmp(tmp[0],para_name) == 0) strcpy(result,tmp[1]); &#125; &#125; fin.close(); &#125; else&#123; cout &lt;&lt; &quot;can&apos;t open config file named &apos;&quot; &lt;&lt; key_path &lt;&lt; &quot;&apos;&quot; &lt;&lt;endl; &#125; &#125;&#125;;#endif // Config_h__ 测试config文件 1234567a=1234b=123asdf#日志区cc=4444d=1 test.cpp 1234567891011121314151617//////////////////////////////////////////////////////////////////////////// test.cpp// Author: joyyzhang// Date: 2017-08#include &quot;Config.h&quot;int main()&#123; Config *cf = new Config(); char result[LINE_LENGTH]; strcpy(result,&quot;&quot;); cf-&gt;Get(&quot;config&quot;,&quot;c&quot;,result); cout &lt;&lt; result &lt;&lt; endl;; system(&quot;pause&quot;); return 0;&#125; 结果 14444 Get函数返回结果为一字符串，可通过atoX函数自行转化为相应的数据类型，如读取的为int类型，可用atoi函数转化等。","tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"用Git上传项目到 GitHub","date":"2017-08-09T11:46:33.437Z","path":"2017/08/09/用Git上传项目到 GitHub/","text":"安装git，git下载网址 输入下列命令配置用户名邮箱 12git config --global user.name &quot;your name&quot;git config --global user.email &quot;your_email@youremail.com&quot; 查看配置 1git config -l 重置配置（重置后需要重启电脑）1git config --unset --global 提交 123git add README.mdgit commit -m &quot;提交注释&quot;git push origin master 更加详细使用可以参见用 Git 上传项目到 GitHub","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"}]},{"title":"缓存模拟器-cache-sim_v1.0","date":"2017-08-09T11:46:33.425Z","path":"2017/08/09/缓存模拟器-cache-sim_v1.0/","text":"代码已经上传至github通过下列命令下载 1git clone https://github.com/zydirtyfish/cache-sim_v1.0.git 进入目录，make进行编译 1make 输入下列命令可运行demo程序 1make run demo程序中分析的是trace文件夹的下的example文件，更多的trace文件可在网站SNIA网站上下载。 缓存所有的配置信息在config文件中，配置文件属性与值之间以等号连接，注释以#开头123456789101112131415161718#缓存替换算法的类型algorithm_type=0#缓存的大小block_num_conf=65536#块大小block_size_conf=4096#写策略write_algorithm_conf=1#日志的开始log_start=0#每次读取的日志数log_num=1#日志文件的前缀log_prefix=./trace/example.csv#懒惰参数PARA=4k=16 程序入口在cache-sim.cpp中的main函数，可以发现，配置文件通过命令行参数传入，即通过下列命令运行1./cache-sim.o config 缓存的初始化12init_cache()//函数读取配置文件并对缓存进行初始化destroy_cache()//销毁缓存 运行缓存main()函数通过调用Run对象的exec()方法运行缓存,exec()函数调用了Algorithm类中的kernel()方法。因此kernel()是整个cache-sim的核心方法。 不同的缓存替换算法都继承了Algorithm这个算法父类，并且实现了父类中的虚函数map_operation()，kernel()函数通过调用不同的map_operation()实现不同的替换策略，这一点类似于flashcache的实现，flashcache实现的就是map_operation()所实现的功能，而kernel()则类似于device-mapper层提供的转发功能。 结果显示","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"cache-sim","slug":"cache-sim","permalink":"http://yoursite.com/tags/cache-sim/"},{"name":"缓存","slug":"缓存","permalink":"http://yoursite.com/tags/缓存/"}]},{"title":"机器学习 scikit_learn使用-1-安装","date":"2017-07-20T14:29:23.885Z","path":"2017/07/20/机器学习 scikit_learn使用-1-安装/","text":"机器学习分类通常，一个学习问题是通过一系列的n个样本数据来学习然后尝试预测未知数据的属性。如果每一个样本超过一个单一的数值，例如多维输入（也叫做多维数据），那么它就拥有了多个特征。 我们可以把学习问题划分为几个大的来别： 监督学习在监督学习中，这些数据自带了我们想要预测的附加属性（scikit-learn监督学习链接），这个问题包括： 分类样本属于属于两类或者多类，我们想从已经被标记的数据中来预测未知数据的类别。一个分类问题的例子就是手写字识别。这个例子的目的是从有些的类别中识别出输入向量的类别。对于分类的另一种想法是作为监督学习的一种分离的表格(不是连续的)，在这个表格中，一个是被限制的类别数量，而且对于每个类别都有N个样例被提供；一个是尝试用正确的类别或者类来标记他们。 回归如果期望的输出是由一个或者更多的连续的变量组成，那么就叫做回归。回归问题的例子将通过一条鲑鱼的年龄和重量预测它的长度。 无监督学习在无监督学习里面，训练数据是由一组没有任何类别标签值的一系列输入向量组成。这种问题的目的是可能可以在这些数据里发现相似的样例组，这些相似的样例被称作聚类。或者在输入空间里决定数据分布，称之为密度估算；或者将数据从高维空间映射到二维或三维空间中，称之为数据可视化问题。（无监督学习链接） 训练集和测试集 机器学习是关于学习数据集的一些属性然后将它们应用到新的数据上。这就是为什么在机器学习中评价一个算法的通常惯例是把数据集切分为两个数据集，其中一个叫做训练集，用来学习数据的属性；另一个叫做测试集，在测试集上测试那些属性。 在Ubuntu上安装Scikit-Learn等Python packages安装PythonUbuntu 14.04 自带了Python 2.7 和Python 3.4，默认使用Python 2.7。所以这里并不需要做什么，只要通过下面的指令看看python是否安装正确。123which pythonpython --version 安装常用packages12345sudo apt-get install libdpkg-perl=1.17.5ubuntu5sudo apt-get install dpkg-devsudo apt-get install build-essential python-dev python-setuptools python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose 安装pip1sudo python get-pip.py 查看已安装的packages，在python中输入： 12import pipprint(pip.get_installed_distributions()) 安装scikit-learn-1-安装1sudo apt-get install python-sklearn","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"scikit_learn","slug":"scikit-learn","permalink":"http://yoursite.com/tags/scikit-learn/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"机器学习 scikit_learn使用-2-demo","date":"2017-07-20T14:29:23.884Z","path":"2017/07/20/机器学习 scikit_learn使用-2-demo/","text":"加载样本数据集scikit-learn带有一些标准的数据集，例如用于分类的iris和digit数据集和用于回归的 boston house prices dataset . 下面，我们打开Python编译器，然后载入iris和digits数据集。我们的符号’$’表示shell提示，’&gt;&gt;&gt;’表示python编译器提示 1234$ python&gt;&gt;&gt; from sklearn import datasets&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; digits = datasets.load_digits() 在digits数据集情况下，digits.data提供了可用于分类数字样本。 1print(digits.data) 并且digits.target给出了digit数据集的真实结果，这些数字是和我们正在学习的每个数字图像相关的数字。 1digits.target 数据总是一些2D数组，shape(n_samples,n_features),尽管原始数据也许有一个不同的形状，就这个digits而言，每一个原始样例是一个shape(8,8)的图像，并且能被访问使用:1digits.images[0] 学习和预测在digits数据集中，给定一幅手写数字的数字图像，任务是预测结果。我们给定的样本有10种类别（是数字0到9），基于此我们建立一个估计方法能够预测我们没有见过的样本属于哪一类。 在scikit-learn中，用于分类的估计模型是一个实现了fit(x,y)方法和predict(T)方法的Python对象。 估计模型的例子是在实现了support vector classification支持向量机的类 sklearn.svm.SVC。估计模型的构造函数带有模型参数，但是目前，我们将估计模型当做一个黑盒子。 123from sklearn import svm clf = svm.SVC(gamma=0.001, C=100.) 选择模型参数在这个例子中，我们这设定了gamma值。可以通过使用网格搜索和交叉验证自动的找出最好的参数值 我们把我们的评估模型命名为clf，作为一个分类器，它现在必须拟合这个模型，也就是它必须从这个模型学习。我们通过将数据集传递给fit函数完成。作为训练集，除了最后一个样本，我们选择其余的所有样本。通过python语句[:-1]选择样本，这条语句将从digits.data中产生一个除了最后一个样本的新数组 1clf.fit(digits.data[:-1], digits.target[:-1]) 现在，我们可以预测新值，尤其是我们可以问分类器在digits数据集中的用来训练分类器时没有使用的最后一个数据是数字几 一个完整的分类问题实例可以通过下面的链接下载，用来作为你运行并且学习的例子 Recognizing hand-written digits 模型持久化可以通过使用python的built-in持久化模型在scikit中保存一个模型，命名pickle:123456789101112131415161718&gt;&gt;&gt; from sklearn import svm&gt;&gt;&gt; from sklearn import datasets&gt;&gt;&gt; clf = svm.SVC()&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; X, y = iris.data, iris.target&gt;&gt;&gt; clf.fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=&apos;auto&apos;, kernel=&apos;rbf&apos;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)&gt;&gt;&gt; import pickle&gt;&gt;&gt; s = pickle.dumps(clf)&gt;&gt;&gt; clf2 = pickle.loads(s)&gt;&gt;&gt; clf2.predict(X[0])array([0])&gt;&gt;&gt; y[0]0 在scikit的特别情况下，使用joblib替换pickle(joblib.dump &amp; joblib.load)会更有趣,它在大数据上是更有效的，但是仅仅只能存入的是字典而不是字符串。 12&gt;&gt;&gt; from sklearn.externals import joblib&gt;&gt;&gt; joblib.dump(clf, &apos;filename.pkl&apos;) 然后你就可以读取上面的pickled模型使用了（通常是在其它的Python程序中）： 1&gt;&gt;&gt; clf = joblib.load(&apos;filename.pkl&apos;)","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"scikit_learn","slug":"scikit-learn","permalink":"http://yoursite.com/tags/scikit-learn/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"kmeans聚类","date":"2017-07-20T14:29:23.882Z","path":"2017/07/20/kmeans聚类/","text":"聚类算法有很多种，比如K-Means，AP聚类，层次聚类(Hierarchical clustering)等等，主要说一下K-Means. kmeans简介K-Means之所以很受欢迎，就是其速度很快并且有很好的可扩展性。K-Means算法是一个重复移动类中心点的过程，把类的中心点，也称重心（centroids），移动到其包含成员的平均位置，然后重新划分其内部成员。 在博文K-Means聚类中有一个很好的里子说明了kmeans的具体工作原理。 kmeans库的使用在scikit_learn库中就有kmeans算法的具体实现，官方说明见连接sklearn.cluster.KMeans 下面代码是具体的使用的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding:UTF-8 -*-import sys ##加载sys这个模块import pickle, subprocess, pwdimport numpy as npfrom sklearn.cluster import KMeans#定义类别数目，参数通过shell指令参数输入cn = sys.argv[1];#print int(cn);#通过numpy库的方法导入测试向量#测试向量的行数表示样本数#测试向量的列数表示参数的维度X=np.loadtxt(&apos;test_vec&apos;)#kmeans算法学习print(&apos;starting learning...&apos;)kmeans = KMeans(n_clusters=int(cn), random_state=0).fit(X)#持久化模型print(&apos;model dumping begin...&apos;)pickle.dump(kmeans , open(&apos;model.pkl&apos;, &apos;wb&apos;))print(&apos;model dumping end...&apos;)#读取模型kmeans = pickle.load(open(&apos;model.pkl&apos;, &apos;rb&apos;))#输出学习的样本都属于哪个类别M = kmeans.predict(X)for j in M: print j#输出各类别的聚类中心cc = kmeans.cluster_centers_;for i in cc: for j in i: print(&apos;%lf\\t&apos;%j) print &apos;\\n&apos;print cc[0];#输出样本到各距离中心的距离总和#结合肘部法则，可以确定最佳的Kdis = kmeans.inertia_ ;print dis;","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"kmeans聚类","slug":"kmeans聚类","permalink":"http://yoursite.com/tags/kmeans聚类/"}]},{"title":"ssdsim学习1","date":"2017-07-20T14:29:23.880Z","path":"2017/07/20/ssdsim学习1/","text":"说明：==下文内容摘自胡洋博士毕业论文。引文通过特殊格式标出== 本章介绍一个自主开发的高准确性、模块化、可配置的固态盘模拟器SSDsim，它能够对高级命令、能耗进行模拟，最终要的一点是它的模拟结果经过了基本的验证。 源码请参见ssdsim 时间模拟事件驱动的模拟器是根据系统内部各个部件的状态改变来修改系统时间，SSDsim就是采用的这种方式的驱动方法； 在高性能固态盘中，存在三类主要的部件，既固态盘中的嵌入式处理器，内存颗粒，和闪存颗粒。处理器的时钟通常大于100MHz，既单个时钟的时间为10纳秒甚至更短；内存的读写操作时间是纳秒级；与此形成对比的是，闪存颗粒的读写时间是微秒级。因此在SSDsim中，只考虑对闪存的读写擦除操作的时间开销，内存的读写时间开销取一个平均值，忽略处理器的时间开销。对于一个独立的读、写、擦除操作，可以根据公式5.1、5.2、5.3获得准确的时间开销。表5.1中介绍了三个公式中出现的变量，表中所列参数具体值取自具体的闪存芯片的数据手册 读操作： 7×tWC+tR+PS×tRC 公式1 写操作：7×tWC +PS×tWC+tPROG 公式2 擦除操作：5×tWC+tERASE 公式3 简称 含义 时间 tR 数据从目标物理页中读到分组的寄存器所消耗的时间 20微妙 tPROG 数据从分组的寄存器写到目标物理页所消耗的时间 200微妙 tERASE 目标物理块的擦除时间 1.5毫秒 tWC 通过数据总线上向寄存器传输一个字节的数据所消耗的时间 25纳秒 tRC 通过数据总线从寄存器向外传输一个字节的数据所消耗的时间 25纳秒 PS 传输的数据量 能耗模拟 因为不同的固态盘所使用的处理器不一样，不管采用什么样硬件结构和软件算法，处理器的能耗基本相同，所以在SSDsim中，不考虑处理器的能耗，只考虑内存和闪存的能耗。 固态盘中需要一定的内存空间存储映射关系和缓存数据，通常采用两种类型的内存，DRAM或者SRAM。SRAM成本较高，容量较低，一般集成在处理器中；DRAM的成本较低，容量较大，一般是独立存在的。DRAM有三种工作状态，激活状态（active）、低功耗状态（standby）、刷新状态（refresh），在对DRAM进行读写操作时，DRAM处于激活状态；当短时间内没有对DRAM进行读写操作时，DRAM进入低功耗状态；当长时间内没有对DRAM有读写操作，DRAM就进入周期性刷新状态。在SSDsim中，每次对内存的操作的时间可以根据读写的数据量计算获得，所以DRAM处于激活状态的能耗可以根据所使用的内存颗粒的电压电流以及内存操作时间计算获得；当DRAM服务完一个读写操作后，随即进入低功耗状态，这段时间是两次读写操作之间的间隔时间，也可以计算获得；DRAM的刷新过程与所服务的负载的时间跨度有关，特定DRAM芯片的刷新周期是一定的，例如64毫秒，在SSDsim中可以根据负载的时间跨度，DRAM的刷新周期，以及DRAM的大小既DRAM刷新单位计算获得DRAM的刷新操作所消耗的能耗。SSDsim通过采用上述的方法，分别获得闪存的能耗和内存的能耗，最终获得所模拟的固态盘服务某个负载所耗费的能量。 模块划分 SSDsim用来模拟固态盘的硬件结构（包括闪存颗粒，固态盘内部通道等），闪存转换层（包括数据缓存区、地址映射、垃圾回收及损耗均衡等）。SSDsim分成三个逻辑模块，既硬件行为模拟层，闪存转换层，和数据缓存层，这三个模块也是固态盘中标准的三个组成部分。 基本流程","tags":[{"name":"ssdsim","slug":"ssdsim","permalink":"http://yoursite.com/tags/ssdsim/"},{"name":"ssd","slug":"ssd","permalink":"http://yoursite.com/tags/ssd/"}]},{"title":"linux设备驱动-3-基本数据结构","date":"2017-07-20T14:29:23.879Z","path":"2017/07/20/linux设备驱动-3-基本数据结构/","text":"引言内核中使用的数据结构有双向链表、hash链表和单向链表，另外，红黑树和基树(radix数)也是内核使用的数据结构。 container是Linux中很重要的一个概念，使用container能实现对象封装。代码如下： 123456#define offsetof(TYPE,MEMBER) ((size_t)&amp;((TYPE *)0)-&gt;MEMBER)#define container_of(ptr,type,member)(&#123;\\ const typeof( ((type *)0)-&gt;member) *_mptr = (ptr);\\ (type *)( (char *)_mptr - offsetof(type,member) );\\&#125;) 通过这种方法巧妙地实现了通过结构的一个成员找到整个结构的地址。内核中大使用了这种方法。 1 双向链表list 是双向链表的一个抽象，它的定义在/include/linux目录下。 定义123struct list_head&#123; struct list_head *next,*prev;&#125; list_head这个结构看起来怪怪的，它竟没有数据域！所以看到这个结构的人第一反应就是我们怎么访问数据？其实list_head不是拿来单独用的，它一般被嵌到其它结构中，如： 123456struct student&#123; int id; char* name; struct list_head list;&#125;; 此时list_head就作为它的父结构中的一个成员了，当我们知道list_head的地址（指针）时，我们可以通过list.c提供的宏 list_entry 来获得它的父结构的地址。 可以参见Linux 内核list_head学习（一） list提供的list_entry使用了container,通过container可以从list找到整个数据对象，这样list就成为了一种通用的数据结构。 12#define list_entry(ptr,type,member)container of(ptr,type,member) 方法1234567LIST_HEAD //定义并初始化一个listlist_add_tail // 在list尾部加一个成员list_del //删除一个list成员list_empty //将list置空list_for_each //遍历链表list_for_each_safe //遍历并且可以删除成员list_for_each_entry //遍历，通过container方法返回结构指针 举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include&lt;linux/init.h&gt;#include&lt;linux/slab.h&gt;#include&lt;linux/module.h&gt;#include&lt;linux/kernel.h&gt;#include&lt;linux/list.h&gt;MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;joystorage&quot;);MODULE_DESCRIPTION(&quot;My First LKM for list_head!&quot;);struct student&#123; int id; char* name; struct list_head list;&#125;;void print_student(struct student*);static int testlist_init(void)&#123; struct student *stu1, *stu2, *stu3, *stu4; struct student *stu; // init a list head LIST_HEAD(stu_head); // init four list nodes stu1 = kmalloc(sizeof(*stu1), GFP_KERNEL); stu1-&gt;id = 1; stu1-&gt;name = &quot;wyb&quot;; INIT_LIST_HEAD(&amp;stu1-&gt;list); stu2 = kmalloc(sizeof(*stu2), GFP_KERNEL); stu2-&gt;id = 2; stu2-&gt;name = &quot;wyb2&quot;; INIT_LIST_HEAD(&amp;stu2-&gt;list); stu3 = kmalloc(sizeof(*stu3), GFP_KERNEL); stu3-&gt;id = 3; stu3-&gt;name = &quot;wyb3&quot;; INIT_LIST_HEAD(&amp;stu3-&gt;list); stu4 = kmalloc(sizeof(*stu4), GFP_KERNEL); stu4-&gt;id = 4; stu4-&gt;name = &quot;wyb4&quot;; INIT_LIST_HEAD(&amp;stu4-&gt;list); // add the four nodes to head list_add (&amp;stu1-&gt;list, &amp;stu_head); list_add (&amp;stu2-&gt;list, &amp;stu_head); list_add (&amp;stu3-&gt;list, &amp;stu_head); list_add (&amp;stu4-&gt;list, &amp;stu_head); // print each student from 4 to 1 list_for_each_entry(stu, &amp;stu_head, list) &#123; print_student(stu); &#125; // print each student from 1 to 4 list_for_each_entry_reverse(stu, &amp;stu_head, list) &#123; print_student(stu); &#125; // delete a entry stu2 list_del(&amp;stu2-&gt;list); list_for_each_entry(stu, &amp;stu_head, list) &#123; print_student(stu); &#125; // replace stu3 with stu2 list_replace(&amp;stu3-&gt;list, &amp;stu2-&gt;list); list_for_each_entry(stu, &amp;stu_head, list) &#123; print_student(stu); &#125; return 0;&#125;static void testlist_exit(void)&#123; printk(KERN_ALERT &quot;*************************\\n&quot;); printk(KERN_ALERT &quot;testlist is exited!\\n&quot;); printk(KERN_ALERT &quot;*************************\\n&quot;);&#125;void print_student(struct student *stu)&#123; printk (KERN_ALERT &quot;======================\\n&quot;); printk (KERN_ALERT &quot;id =%d\\n&quot;, stu-&gt;id); printk (KERN_ALERT &quot;name=%s\\n&quot;, stu-&gt;name); printk (KERN_ALERT &quot;======================\\n&quot;);&#125;module_init(testlist_init);module_exit(testlist_exit);","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"设备驱动","slug":"设备驱动","permalink":"http://yoursite.com/tags/设备驱动/"}]},{"title":"flashcache-4-修改与测试","date":"2017-07-20T14:29:23.877Z","path":"2017/07/20/flashcache-4-修改与测试/","text":"系统安装由于flashcache在最新版本的ubuntu上会有兼容性问题（新版本ubuntu的linux内核版本高于4，ioctl被删除了，因此新版本ubuntu安装flashcache需要打补丁，比较麻烦），这里选用旧版本的ubuntu 14.10(utopic) 安装ubuntu14.10 utopic 这个版本的ubuntu官网之前的源失效了，可以修改源文件（具体参见求助 13.04 更新及升级） 1sudo gedit /etc/apt/sources.list 然后修改为下面的源1deb http://old-releases.ubuntu.com/ubuntu/ utopic main restricted universe multiverse flashcache安装这里有篇博文推荐一下Flashcache基本使用及注意事项 在官网下载文件解压github-flashcache 进入文件夹并输入 SSD:/dev/sdc SAS:/dev/sdb2 创建设备名为cachedev的flashcache 1234567891011121314151617181920##安装gitsudo apt-get install git##编译安装makemake install##加载flashcache模块modprobe flashcachelsmod | grep flashcache##创建缓存设备flashcache_create -p back -b 4k cachedev /dev/sdc /dev/sdb2##格式化分区mkfs.ext4 /dev/mapper/cachedev##挂载分区mount /dev/mapper/cachedev /data flashcache_create相关参数说明：-p:缓存模式 writeback(数据先写到SSD，随后写到普通硬盘)，writethrough(数据同时写到SSD和普通硬盘)，writearound（数据绕过SSD，直接写到普通硬盘）三种，三种模式的所有读都会被缓存到flashcache可以通过dev.flashcache..cache_all参数调整 -s：缓存大小，可选项，如果未指定则整个SSD设备被用于缓存，默认的计数单位是扇区（sectors）,但是可以接受k/m/g单位。 -b：指定块大小，可选项，默认为4KB，必须为2的指数。默认单位为扇区。也可以用K作为单位，一般选4KB。 -f：强制创建，不进行检查 -m：设备元数据块大小，只有writeback需要存储metadata块，默认4K 销毁Flashcache12345flashcache_destroy /dev/sdc ##这种方式删除writeback模式的flashcache时会将SSD上的所有数据删除包括脏数据,建议使用dmsetup命令（device-mapper软件包）删除，会自动将脏数据写入磁盘dmsetup remove cachedev##卸载模块rmmod flashcache 其他命令123flashcache_create ##查看帮助flashcache_load /dev/sdc cachedev ##(系统重启时使用来加载已经创建过的缓存设备cachedev)mount /dev/mapper/cachedev /data ##创建好的flashcache设备是块设备，可格式文件系统后挂在使用,也可以继续对其分区等 Flashcache参数优化输入下面命令进行查看1sysctl dev.flashcache 默认参数设置 123456789101112131415161718192021dev.flashcache.sdc+sdb2.io_latency_hist = 0dev.flashcache.sdc+sdb2.do_sync = 0dev.flashcache.sdc+sdb2.stop_sync = 0dev.flashcache.sdc+sdb2.dirty_thresh_pct = 20dev.flashcache.sdc+sdb2.max_clean_ios_total = 4dev.flashcache.sdc+sdb2.max_clean_ios_set = 2dev.flashcache.sdc+sdb2.do_pid_expiry = 0dev.flashcache.sdc+sdb2.max_pids = 100dev.flashcache.sdc+sdb2.pid_expiry_secs = 60dev.flashcache.sdc+sdb2.reclaim_policy = 0dev.flashcache.sdc+sdb2.zero_stats = 0dev.flashcache.sdc+sdb2.fast_remove = 0dev.flashcache.sdc+sdb2.cache_all = 1dev.flashcache.sdc+sdb2.fallow_clean_speed = 2dev.flashcache.sdc+sdb2.fallow_delay = 900dev.flashcache.sdc+sdb2.skip_seq_thresh_kb = 0dev.flashcache.sdc+sdb2.clean_on_read_miss = 0dev.flashcache.sdc+sdb2.clean_on_write_miss = 0dev.flashcache.sdc+sdb2.lru_promote_thresh = 2dev.flashcache.sdc+sdb2.lru_hot_pct = 75dev.flashcache.sdc+sdb2.new_style_write_merge = 0 修改参数的方法 1sysctl -w dev.flashcache.sdc+sdb2.reclaim_policy=1 ###缓存回收策略，0：FIFO，1：LRU，可动态调整 Flashcache状态监控12345678910dmsetup status cachedevdmsetup table cachedev 错误日志报告 /proc/flashcache/sdc+sdb2/flashcache_errors 状态报告/proc/flashcache/sdc+sdb2/flashcache_stats 亦可使用flashstat命令实时查看 推荐IO测试工具fio、iozone 更多参考https://github.com/geekwolf/sa-scripts/blob/master/devops.md fio安装1sudo apt-get install fio FIO的用法FIO分顺序读，随机读，顺序写，随机写，混合随机读写模式。 这是一个顺序读的模式：fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=1000 -group_reporting -name=mytest 说明：1234567891011121314151617filename=/dev/sdb1 测试文件名称，通常选择需要测试的盘的data目录。direct=1 测试过程绕过机器自带的buffer。使测试结果更真实。rw=randwrite 测试随机写的I/Orw=randrw 测试随机写和读的I/Obs=16k 单次io的块文件大小为16kbsrange=512-2048 同上，提定数据块的大小范围size=5G 本次的测试文件大小为5g，以每次4k的io进行测试。numjobs=30 本次的测试线程为30个.runtime=1000 测试时间为1000秒，如果不写则一直将5g文件分4k每次写完为止。ioengine=psync io引擎使用pync方式rwmixwrite=30 在混合读写的模式下，写占30%group_reporting 关于显示结果的，汇总每个进程的信息。lockmem=1G 只使用1g内存进行测试。zero_buffers 用0初始化系统buffer。nrfiles=8 每个进程生成文件的数量。 一个完整的例子新建一个虚拟的ssd设备（用内存建立） mkssd.sh1234567891011#!/bin/bashssd_size=$1# 限制tmpfs最大不超过10G，避免耗尽内存(测试机器有24G物理内存)$sudo mount tmpfs /dev/shm -t tmpfs -o size=10240m# 创建ssd_size大小的文件，用来模拟flash设备$dd if=/dev/zero of=/dev/shm/ssd.img bs=1024k count=$ssd_size# 将文件模拟成块设备$sudo losetup /dev/loop0 /dev/shm/ssd.img 建立flashcache设备 mkcache.sh 1234567891011121314151617181920212223242526272829303132333435363738394041#!/bin/bash##缓存名cache_name=$1##替换策略cache_policy=$2##安装gitsudo apt-get install git##编译安装makemake install##加载flash模块modprobe flashcachelsmod | grep flashcache##创建缓存设备flashcache_create -p back -b 4k $cache_name /dev/loop0 /dev/sdb##格式化分区mkfs.ext4 /dev/mapper/$cache_name##挂载分区mount /dev/mapper/$cache_name /data##设置缓存替换算法sysctl -w dev.flashcache.loop0+sdb.reclaim_policy=$cache_policy##查看缓存统计信息cat /proc/flashcache/loop0+sdb/flashcache_stats##销毁缓存dmsetup remove $cache_name##卸载flashcache模块rmmod flashcache##安装fiosudo apt-get install fio##测试fio -filename=/dev/mapper/$cache_name -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=100 -group_reporting -name=mytest 运行结果截图 fio测试结果 1fio -filename=/dev/sdb -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=200G -numjobs=30 -runtime=100 -group_reporting -name=mytest flashcache统计结果 1cat /proc/flashcache/loop0+sdb/flashcache_stats","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"}]},{"title":"shell脚本循环","date":"2017-07-11T07:56:18.525Z","path":"2017/07/11/shell脚本循环/","text":"shell语法真是头疼！！每次都忘，每次都要去查！！ 转载自Shell脚本中循环语句for,while,until用法 Bash Shell中主要提供了三种循环方式：for、while和until。 一、for循环 for循环的运作方式，是讲串行的元素意义取出，依序放入指定的变量中，然后重复执行含括的命令区域（在do和done 之间），直到所有元素取尽为止。 其中，串行是一些字符串的组合，彼此用$IFS所定义的分隔符（如空格符）隔开，这些字符串称为字段。for的语法结构如下： 1234for 变量 in 串行do 执行命令done 用例1 用for循环在家目录下创建aaa1-aaa10，然后在aaa1-aaa10创建bbb1-bbb10的目录 123456789101112#!/bin/bashfor k in $( seq 1 10 )do mkdir /home/kuangl/aaa$&#123;k&#125; cd /home/kuangl/aaa$&#123;k&#125; for l in $( seq 1 10 ) do mkdir bbb$&#123;l&#125; cd /home/kuangl/aaa$&#123;k&#125; done cd ..done 用例2 列出var目录下各子目录占用磁盘空间的大小。1234567#!/bin/bashDIR=&quot;/var&quot;cd $DIRfor k in $(ls $DIR)do [ -d $k ] &amp;&amp; du -sh $kdone 二、while循环while循环的语法： 1234while 条件测试do 执行命令done 用例1 while循环，经典的用法是搭配转向输入，读取文件的内容，做法如下： 12345#!/bin/bashwhile read kuangldo echo $&#123;kuangl&#125;done &lt; /home/kuangl/scripts/testfile 说明： 行2，使用read有标准输入读取数据，放入变量kuangl中，如果读到的数据非空，就进入循环。 行4，把改行数据显示出来 行5，将/home/kuangl/scripts/testfile的内容转向输入将给read读取。 用例2： 123456789#!/bin/bashdeclare -i i=1declare -i sum=0while ((i&lt;=10))do let sum+=i let ++idoneecho $sum 说明： 行2-3，声明i和sum为整数型 行4，如果条件测试：只要i值小于或者等于10，就执行循环。 行6，sum+=i和sum=sum+i是一样的，sum累加上i。 行7，i的值递增1，此行是改变条件测试的命令，一旦i大于10，可终止循环。 行8，遇到done，回到行6去执行条件测试 行9，显示sum的值为55 三、until循环while循环的条件测试是测真值，until循环则是测假值。until循环的语法： 1234until 条件测试do 执行命令done 用例1： 123456789#!/bin/bashdeclare -i i=10declare -i sum=0until ((i&gt;10))do let sum+=i let ++idoneecho $sum","tags":[{"name":"转载","slug":"转载","permalink":"http://yoursite.com/tags/转载/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"},{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"flashcache-3-底层存储模块","date":"2017-05-24T01:37:39.432Z","path":"2017/05/24/flashcache-3-底层存储模块/","text":"转载自Jessica程序猿博客 flashcache中跟底层存储模块分为以下两类： 1）磁盘跟内存的交互 2）磁盘跟磁盘之前的交互 比如说读不命中时就是直接从磁盘读，属于第1种情况，那读命中呢？也是属于第1种情况，不过这时候是从SSD读。磁盘跟磁盘之间交互是用于写脏数据，将SSD中脏cache块拷贝到磁盘上去。现在介绍下两种情况使用的接口函数，这样后面在看读写流程时看到这两个函数就十分亲切了，并且清楚地知道数据是从哪里流向哪里。 底层存储调用的两种方法对于情况1主要是两个函数dm_io_async_bvec和flashcache_dm_io_async_vm。 dm_io_async_bvec 在文件flashcache_main.c中 12345678910111213141516171819202122232425262728#if LINUX_VERSION_CODE &gt;= KERNEL_VERSION(2,6,22)int dm_io_async_bvec(unsigned int num_regions, #if LINUX_VERSION_CODE &gt;= KERNEL_VERSION(2,6,26) struct dm_io_region *where, #else struct io_region *where, #endif int rw, struct bio *bio, io_notify_fn fn, void *context)&#123; struct dm_io_request iorq; iorq.bi_rw = rw;#if LINUX_VERSION_CODE &gt;= KERNEL_VERSION(3,14,0) iorq.mem.type = DM_IO_BIO; iorq.mem.ptr.bio = bio; #else iorq.mem.type = DM_IO_BVEC; iorq.mem.ptr.bvec = bio-&gt;bi_io_vec + bio-&gt;bi_idx;#endif iorq.notify.fn = fn; iorq.notify.context = context; iorq.client = flashcache_io_client; return dm_io(&amp;iorq, num_regions, where, NULL);&#125;#endif flashcache_dm_io_async_vm在文件flashcache_subr.c中 12345678910111213141516171819202122232425262728293031#if LINUX_VERSION_CODE &gt;= KERNEL_VERSION(2,6,22)int flashcache_dm_io_async_vm(struct cache_c *dmc, unsigned int num_regions, #if LINUX_VERSION_CODE &lt; KERNEL_VERSION(2,6,26) struct io_region *where, #else struct dm_io_region *where, #endif int rw, void *data, io_notify_fn fn, void *context)&#123; unsigned long error_bits = 0; int error; struct dm_io_request io_req = &#123; .bi_rw = rw, .mem.type = DM_IO_VMA, .mem.ptr.vma = data, .mem.offset = 0, .notify.fn = fn, .notify.context = context, .client = flashcache_io_client, &#125;; error = dm_io(&amp;io_req, 1, where, &amp;error_bits); if (error) return error; if (error_bits) return error_bits; return 0;&#125;#endif 上面两个函数都使用struct dm_io_request 来包装了请求，其中的只有两种请求的类型是不一样的，第一个函数对应的是DM_IO_BVEC，第二个函数是DM_IO_VMA。 其实我开始一直不明白，为什么要使用这两个函数让硬盘与内存打交道，不过后来看了dm_io发现其中的io服务类型有多种不同类型，这两个函数的调用分别对应不同的io类型。下面先看一下dm_io相关的数据结构。 dm_io以上种方法都调用了更底层的dm_io函数，dm-io为device mapper提供同步或者异步的io服务。 使用dm-io必须设置dm_io_region结构（2.6.26版本以前叫io_region），该结构定义了io操作的区域，读一般针对一个dm_io_region区，而写可以针对一组dm_io_region区。 12345678910111213//2.6.26版本以后struct dm_io_region &#123; struct block_device *bdev; sector_t sector; sector_t count; /* If this is zero the region is ignored. */&#125;;//2.6.26版本以前struct io_region &#123; struct block_device *bdev; sector_t sector; sector_t count;&#125;; Dm-io 可以从一个io_region中读取或者写入到一个或者多个io_region中去。一个io_region结构数组指定了写入到多个区域。 dm_io_mem_type类型dm-io一共有四种dm_io_mem_type类型（老一点的内核版本只有前面三种，Flashcache主要使用DM_IO_BVEC）: 123456789101112131415161718enum dm_io_mem_type &#123; DM_IO_PAGE_LIST,/* Page list */ DM_IO_BVEC, /* Bio vector */ DM_IO_VMA, /* Virtual memory area */ DM_IO_KMEM, /* Kernel memory */&#125;;struct dm_io_memory &#123; enum dm_io_mem_type type; union &#123; struct page_list *pl; struct bio_vec *bvec; void *vma; void *addr; &#125; ptr; unsigned offset;&#125;; Dm-io 提供同步和异步I/O服务。老一点的内核它提供了3种I/O服务，每种服务都有一个同步和一个异步的版本。 DM_IO_PAGE_LIST第一个I/O服务类型使用了一串内存页作为缓冲区，伴随着一个首页面的偏移。 12345678910struct page_list &#123; struct page_list *next; struct page *page;&#125;;int dm_io_sync(unsigned int num_regions, struct io_region *where, int rw, struct page_list *pl, unsigned int offset, unsigned long *error_bits);int dm_io_async(unsigned int num_regions, struct io_region *where, int rw, struct page_list *pl, unsigned int offset, io_notify_fn fn, void *context); DM_IO_BVEC第二种I/O服务类型把一组bio载体当着I/O的数据缓冲。如果调用者提前拼装了bio，这个服务可以很顺利地完成。但是需要将不同的bio页指向不同的设备。 123456int dm_io_sync_bvec(unsigned int num_regions, struct io_region *where, int rw, struct bio_vec *bvec, unsigned long *error_bits);int dm_io_async_bvec(unsigned int num_regions, struct io_region *where, int rw, struct bio_vec *bvec, io_notify_fn fn, void *context); DM_IO_VMA第三种I/O服务类型把一个指向虚拟动态内存缓冲区的的指针当作I/O的数据缓冲。如果调用者需要在很大的块设备上进行I/O操作又不想分配大量的个人内存页，那么这种服务可以胜任。 1234int dm_io_sync_vm(unsigned int num_regions, struct io_region *where, int rw, void *data, unsigned long *error_bits);int dm_io_async_vm(unsigned int num_regions, struct io_region *where, int rw, void *data, io_notify_fn fn, void *context); 异步I/O服务的调用者必须包含一个完成的回调函数和一个指向一些这个I/O内容数据的指针。 typedef void (io_notify_fn)(unsigned long error, void context); 这个”error”参数，就像这个”*error”参数在任何同步版本中一样，在这个回调函数中就象一个位集合（而不是一个简单的错误值）。 在写I/O到多个目标区域的情况下，这个位集合允许dm-io说明在每个单独的区域上的成功或者失败。在使用任何dm-io服务之前，用户必须调用dm_io_get()、同时指定他们想要的页数来执行I/O.DM-io将尝试着更改自己的内存池的大小来确认在执行i/o时为了避免不必要的等待而有足够的页面来供给。当用户完成了使用I/O服务，他们将调用dm_io_put(),并指定和给dm_io_get()的相同数量的页面。 dm-io通过dm_io_request结构来封装请求的类型，如果设置了dm_io_notify.fn则是异步IO，否则是同步IO。 123456struct dm_io_request &#123; int bi_rw; /* READ|WRITE - not READA */ struct dm_io_memory mem; /* Memory to use for io */ struct dm_io_notify notify; /* Synchronous if notify.fn is NULL */ struct dm_io_client *client; /* Client memory handler */&#125;; 使用dm_io服务前前需要通过dm_io_client_create函数（在2.6.22版本前是dm_io_get）先创建dm_io_client结构，为dm-io的执行过程中分配内存池。使用dm-io服务完毕后，则需要调用dm_io_client_destroy函数（在2.6.22版本前是dm_io_put）释放内存池。 1234struct dm_io_client &#123; mempool_t *pool; struct bio_set *bios;&#125;; 在flashcache中，首先在flashcache_conf.c文件中的flashcache_init函数中使用flashcache_io_client = dm_io_client_create()新建，然后再在dm_io_async_bvec等函数中将flashcache_io_client传给struct dm_io_request iorq的client成员。 dm-io函数执行具体的io请求。 1234567891011121314151617int dm_io(struct dm_io_request *io_req, unsigned num_regions, struct dm_io_region *where, unsigned long *sync_error_bits)&#123; int r; struct dpages dp; r = dp_init(io_req, &amp;dp); if (r) return r; if (!io_req-&gt;notify.fn) return sync_io(io_req-&gt;client, num_regions, where, io_req-&gt;bi_rw, &amp;dp, sync_error_bits); return async_io(io_req-&gt;client, num_regions, where, io_req-&gt;bi_rw, &amp;dp, io_req-&gt;notify.fn, io_req-&gt;notify.context);&#125; 对于情况2磁盘跟磁盘之前的交互。这种情况只用于将ssd中脏块写入disk中。 123int dm_kcopyd_copy(struct dm_kcopyd_client *kc, struct dm_io_region *from, unsigned int num_dests, struct dm_io_region *dests, unsigned int flags, dm_kcopyd_notify_fn fn, void *context) 第一个参数dm_kcopyd_client，在使用kcopyd异步拷贝服务时，必须先创建一个对应的client，首先要分配“kcopyd客户端”结构，调用函数如下： kcopyd_client_create(FLASHCACHE_COPY_PAGES, &amp;flashcache_kcp_client); 创建dm_kcopyd_client结构(在flashcache_conf.c文件中的flashcache_init函数中使用)。 第二个参数dm_io_region是源地址，第四个参数是目的地址，定义如下struct dm_io_region { struct block_device bdev; sector_t sector; sector_t count; / If this is zero the region is ignored. */};dm_kcopyd_notify_fn fn是kcopyd处理完请求的回调函数context 是回调函数参数，在flashcache都设置对应的kcached_job。","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"},{"name":"转载","slug":"转载","permalink":"http://yoursite.com/tags/转载/"}]},{"title":"flashcache-2-调度模块及IO流程","date":"2017-05-23T11:08:07.787Z","path":"2017/05/23/flashcache-2-调度模块及IO流程/","text":"device mapper调度模块得先说说Device Mapper device mapper简介 Device Mapper（DM）是Linux2.6全面引入的块设备新构架，通过DM可以灵活地管理系统中所有的真实或虚拟的块设备。 Device mapper在内核中作为一个块设备驱动被注册,包含三个重要的对象概念 mapped device 逻辑抽象,内核向外提供的逻辑设备，它通过映射表描述的映射关系和target device建立映射。 target device Target device 表示的是mapped device所映射的物理空间段，对mapped device所表示的逻辑设备来说，就是该逻辑设备映射到的一个物理设备。 映射表 从Mapped device到一个target device的映射表由一个多元组表示，该多元组由表示mappeddevice逻辑的起始地址、范围、和表示在targetdevice所在物理设备的地址偏移量以及target类型等变量组成 具体如下图 device mapper创建过程1、根据内核向用户空间提供的ioctl 接口传来的参数，用dm-ioctl.c文件中的dev_create函数创建相应的mapped device结构。这个过程很简单，主要是向内核申请必要的内存资源，包括mapped device和为进行IO操作预申请的内存池，通过内核提供的blk_queue_make_request函数注册该mapped device对应的请求队列dm_request。并将该mapped device作为磁盘块设备注册到内核中。 2、调用dm_hash_insert将创建好的mapped device插入到device mapper中的一个全局hash表中，该表中保存了内核中当前创建的所有mapped device。 3、用户空间命令通过ioctl调用table_load函数，该函数根据用户空间传来的参数构建指定mapped device的映射表和所映射的target device。该函数先构建相应的dm_table、dm_target结构，再调用dm-table.c中的dm_table_add_target函数根据用户传入的参数初始化这些结构，并且根据参数所指定的target类型，调用相应的target类型的构建函数ctr在内存中构建target device对应的结构，然后再根据所建立的dm_target结构更新dm_table中维护的B树。上述过程完毕后，再将建立好的dm_table添加到mapped device的全局hash表对应的hash_cell结构中。 4、最后通过ioctl调用do_resume函数建立mapped device和映射表之间的绑定关系，事实上该过程就是通过dm_swap_table函数将当前dm_table结构指针值赋予mapped_device相应的map域中，然后再修改mapped_device表示当前状态的域。通过上述的4个主要步骤，device mapper在内核中就建立一个可以提供给用户使用的mapped device逻辑块设备。 device mapper具体结构 IO流程下面这一段转载自斯巴达第二季博客 一般来说，我们对磁盘的read和write最后都会走到kernel里的submit_bio函数，也就是把io请求变成一个个的bio（bio的介绍看这里），bio是linux内核里文件系统层和block层之间沟通的数据结构（有点像sk_buffer之于网络），到了block层以后，一般是先做generic_make_request把bio变成request，怎么个变法？如果几个bio要读写的区域是连续的，就攒成一个request（一个request下挂多个连续bio，就是通常说的“合并bio请求”）；如果这个bio跟其它bio都连不上，那它自己就创建一个新的request，把自己挂到这个request下。合并bio请求也是有限度的，如果这些连续bio的访问区域加起来超过了一定的大小（在/sys/block/xxx/queue/max_sectors_kb里设置）,那么就不能再合并成一个request了。 合并后的request会放入每个device对应的queue（一个机械硬盘即使有多个分区，也只有一个queue），之后，磁盘设备驱动程序通过调用peek_request从queue里取出request，进行下一步的处理。 之所以要把多个bio合并成一个request，是因为机械硬盘在顺序读写时吞吐最大。如果我们换成SSD盘，合并这事儿就没那么必要了，这一点是可选的，在实现设备驱动时，厂商可以选择从kernel的queue里取request，也可以选择自己实现queue的make_request_fn方法，直接拿文件系统层传过来的bio来进行处理（fusionio好像就是这么做的）。 我曾经弱弱的问过涛哥：既然bio有bio_vec结构指向多个page，那么为什么不干脆把多个bio合并成一个bio呢？何必要多一个request数据结构那么麻烦？涛哥答曰：每个bio有自己的end_io回调，一个bio结束，就会做自己对应的收尾工作，如果你合并成一个bio了，就丧失了这种灵活性。 linux kernel有一个device mapper框架（以下简称dm框架），linux上的软RAID、multipath等都是通过此框架实现的。dm可以将多个设备聚合成一个虚拟设备提供给用户使用，其原理就是把这个虚拟设备的make_request_fn方法实现成了自己的dm_request，这样所有发往这个虚拟设备的bio都会走进dm_request，然后dm通过判断这个虚拟设备是基于request（request based）的还是基于bio（bio based）的来分别处理： 如果虚拟设备是request based，则和磁盘设备一样走generic_make_request把bio合并成request（如上），注意这些request最后放到的是虚拟设备的queue里，等到虚拟设备通过kblockd调用dm_request_fn时，dm_request_fn里会调用peek_request，从虚拟设备的queue里拿出request，将其clone（clone后的request里的bio指向的page是同一个page，只是分配了新的bio和新的request），然后调用map_request对request做映射（map_request里把map_rq接口暴露给了使用dm框架的开发者），最后把clone后的request发向低层的真实设备。 如果虚拟设备是bio based，就更简单了，调用_dm_request函数，一样要clone bio，然后调用map_bio对bio做映射（map_bio里把map暴露给了使用dm框架的开发者），最后把clone后的bio也是通过generic_make_request发向低层的真实设备，这次generic_make_request生成的request就是放在真实设备的queue里了，这是与request based的不同之处。 flashcache是基于dm框架实现的，很自然的，是把一个SSD盘和一个机械硬盘聚合成一个虚拟设备供用户使用。flashcache把cache（指SSD盘）分为多个set，每个set里有多个block（默认一个block是4KB，一个set包含512个block，即2MB），set里的block是用lru链表组织起来的，每个block还记录了自己存放的是disk的哪个sector起始的位置里对应的内容（这个起始的sector编号在flashcache的文档里被称为dbn）。disk（这里指机械硬盘）也虚拟的分为多个set只是为了方便做hash。hash算法非常简单，先看访问的是disk的什么位置，相当于在disk的哪个set里，然后模上cache里的set数，结果就是在cache里对应的set编号了。找到cache对应的set后，继续在set的lru表里挨个儿block的比对dbn号，对上了就成功，对不上说明cache里没有缓存要读取的disk内容。 例如cache大小为10G，disk大小为100G，用户要读取磁盘上偏移54321MB处的2K内容，那么首先对54321MB这个位置做hash，2MB一个set，对应的set number是27160，cache的总set数为5120，那么 27160 mod 5120 结果为1560，也就是说应该去cache的第1560个set中去找，然后来到cache的1560 set里用 dbn 111249408 遍历查找lru。其实，新版本的flashcache在组相联算法上做了改进，如下。 1234567891011121314151617181920212223242526272829/* * Map a block from the source device to a block in the cache device. */unsigned long hash_block(struct cache_c *dmc, sector_t dbn)&#123; unsigned long set_number, value; int num_cache_sets = dmc-&gt;size &gt;&gt; dmc-&gt;assoc_shift; /* * Starting in Flashcache SSD Version 3 : * We map a sequential cluster of disk_assoc blocks onto a given set. * But each disk_assoc cluster can be randomly placed in any set. * But if we are running on an older on-ssd cache, we preserve old * behavior. */ if (dmc-&gt;on_ssd_version &lt; 3 || dmc-&gt;disk_assoc == 0) &#123; value = (unsigned long) (dbn &gt;&gt; (dmc-&gt;block_shift + dmc-&gt;assoc_shift)); &#125; else &#123;//新版本对flashcache组相联的改进 /* Shift out the low disk_assoc bits */ value = (unsigned long) (dbn &gt;&gt; dmc-&gt;disk_assoc_shift); /* Then place it in a random set */ value = jhash_1word(value, 0xbeef); &#125; set_number = value % num_cache_sets; DPRINTK(&quot;Hash: %llu(%lu)-&gt;%lu&quot;, dbn, value, set_number); return set_number;&#125; flashcache主要是实现了dm框架暴露出来的map接口（参考flashcache_map函数），收到bio后，先做hash，然后在cache（这里指SSD盘）里查找: A. 如果是读bio1 如果查找成功，直接将结果返回2 如果查找失败，则找set内空闲的block（如果没有空闲的，则用最“旧”的block），直接读取disk里对应的内容返回给用户，返给用户后设置延时任务将读取的内容放入这个空闲block里 B. 如果是写bio（我们仅列举writeback情况）1 如果查找成功，拿到对应的block2 如果查找失败，拿到对应set里最“旧”的block直接将数据写入此block，返回给用户（用户的write系统调用就可以返回了），完成后将该block的状态设为DIRTY并设置延时任务，任务内容为将cache里的内容写往disk（这样既能让用户的写请求迅速完成，又能一定程度保证数据最终被写往disk）。延时任务完成后，便可以去掉DIRTY标记了。 flashcache还会不时的将cache的set里长期没被访问的DIRTY的block写往disk，以保证有足够多的干净的block供以后使用。这个“不时的”不是靠定时器实现的，而是通过在flashcache_write_miss、flashcache_read_miss等函数里调用flashcache_clean_set来做到的。 补充两个问题 最后补充一下相关的代码 dm.c 代码可以参见 dm-io.h、bio.h以及device-mapper.h等头文件一般在/usr/src/$(shell uname -r)/include/linux文件夹下 我们进一步看一下上述三个对象在代码中的具体实现，dm.c 文件定义的 mapped_device 结构用于表示 mapped device，它主要包括该 mapped device 相关的锁，注册的请求队列和一些内存池以及指向它所对应映射表的指针等域。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/* * Work processed by per-device workqueue. */struct mapped_device &#123; struct rw_semaphore io_lock; struct mutex suspend_lock; rwlock_t map_lock; atomic_t holders; atomic_t open_count; unsigned long flags; struct request_queue *queue; unsigned type; /* Protect queue and type against concurrent access. */ struct mutex type_lock; struct target_type *immutable_target_type; struct gendisk *disk; char name[16]; void *interface_ptr; /* * A list of ios that arrived while we were suspended. */ atomic_t pending[2]; wait_queue_head_t wait; struct work_struct work; struct bio_list deferred; spinlock_t deferred_lock; /* * Processing queue (flush) */ struct workqueue_struct *wq; /* * The current mapping. */ struct dm_table *map; /* * io objects are allocated from here. */ mempool_t *io_pool; struct bio_set *bs; /* * Event handling. */ atomic_t event_nr; wait_queue_head_t eventq; atomic_t uevent_seq; struct list_head uevent_list; spinlock_t uevent_lock; /* Protect access to uevent_list */ /* * freeze/thaw support require holding onto a super block */ struct super_block *frozen_sb; struct block_device *bdev; /* forced geometry settings */ struct hd_geometry geometry; /* sysfs handle */ struct kobject kobj; /* zero-length flush that will be cloned and submitted to targets */ struct bio flush_bio;&#125;; Mapped device 对应的映射表是由 dm_table.c 文件中定义的 dm_table 结构表示的，该结构中包含一个 dm_target结构数组，在 dm_table 结构中将这些dm_target 按照 B 树的方式组织起来方便 IO 请求映射时的查找操作。 12345678910111213141516171819202122232425262728293031struct dm_table &#123; struct mapped_device *md; atomic_t holders; unsigned type; /* btree table */ unsigned int depth; unsigned int counts[MAX_DEPTH]; /* in nodes */ sector_t *index[MAX_DEPTH]; unsigned int num_targets; unsigned int num_allocated; sector_t *highs; struct dm_target *targets; /* * Indicates the rw permissions for the new logical * device. This should be a combination of FMODE_READ * and FMODE_WRITE. */ fmode_t mode; /* a list of devices used by this table */ struct list_head devices; /* events get handed up using this callback */ void (*event_fn)(void *); void *event_context; struct dm_md_mempools *mempools; &#125; dm_target 结构描述了 mapped_device 到它某个 target device 的映射关系，具体记录该结构对应 target device 所映射的 mapped device 逻辑区域的开始地址和范围，同时还包含指向具体 target device 相关操作的 target_type 结构的指针。 123456789101112131415161718192021222324252627struct dm_target &#123; struct dm_table *table; struct target_type *type; /* target limits */ sector_t begin; sector_t len; /* Always a power of 2 */ sector_t split_io; /* * A number of zero-length barrier requests that will be submitted * to the target for the purpose of flushing cache. * * The request number will be placed in union map_info-&gt;flush_request. * It is a responsibility of the target driver to remap these requests * to the real underlying devices. */ unsigned num_flush_requests; /* target specific data */ void *private; /* Used to provide an error string from the ctr */ char *error; &#125; flashcache中target类型对应target device的结构是struct cache_c","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"}]},{"title":"flashcache-1-入门与安装","date":"2017-05-23T07:07:40.406Z","path":"2017/05/23/flashcache-1-入门与安装/","text":"最近，由于项目需要，在做关于flashcache的一些工作，主要涉及模块组织、元数据管理及数据分布、读写流程分析、数据在磁盘和cache(SSD)之间的调度、缺点及可优化方向等一些方面的分析研究。 特别是，要深入下去的话，会涉及到整个Linux系统栈的各个层次，从文件系统、磁盘缓存、通用块层、驱动层，以及DM的工作流程（细节），也遇到了很多问题，像DM层基于split_bio如何做拆分，在拆分中的边界问题等，不可能一下子解决，也趁此机会，记录下心里的困惑。 flashcache，是facebook技术团队开发的新开源项目，主要目的是用SSD硬盘来缓存数据以加速MySQL的一个内核模块。可以看到，它最初是用来做数据库加速，但同时，它也被作为通用的缓存模块而设计，能够用于任何搭建在块设备上的应用程序。 工作原理基于Device Mapper，它将快速的SSD硬盘和普通的硬盘映射成一个带缓存的逻辑块设备，作为用户操作的接口。用户直接对这个逻辑设备执行读写操作，而不直接对底层的SSD或者普通硬盘操作。如果对底层的这些块设备操作，那么会失去作为一个整体提供的缓存功能。 内核层次flashcache，它是通过在文件系统和块设备驱动层中间增加一缓存层次实现的，这里不得不提到DM层的映射机制。由于DM是作为虚拟的块设备驱动在内核中被注册的，它不是一个真实的设备驱动，不能完成bio的处理，因此，它主要是基于映射表对bio进行分解、克隆和重映射，然后，bio到达底层真实的设备驱动，启动数据传输。在Devicemapper中，引入了target_driver，每个target_driver由target_type类型描述，代表了一类映射，它们分别用来具体实现块设备的映射过程。通过调用某一target_driver的map方法，来映射从上层分发下来的bio，也即是，找到正确的目标设备，并将bio转发到目标设备的请求队列，完成操作。flashcache_target就是这样一个新的target_driver（作为一个新的映射类 型，target_type是必须的），以模块化的方式加入到了DM层。 逻辑架构从源代码层次分析，可以将flashcache分为这个四个模块，调度模块（也称‘读写模块’）、逻辑处理模块（也称“读写后处理模块”）、底层存储模块、以及后台清理模块，它们都是基于SSD Layout实现的，构建在SSD布局（后面会分析）之上。 其中，调度模块，在代码中对应flashcache_map映射函数，它是flashcache缓存层次数据入口，所以到达逻辑设备的读写请求，最终都会经过DM层的处理，通过flashcache_map进入调度模块。称之为“调度”，主要是指，接收到数据后，它会根据bio请求的读写类型、是否命中缓存等因素，选择不同的处理分支，如flashcache_read/write或者flashcache_uncached_io，在read和write中会选择是flashcache_read_hit/miss还是flashcache_write_hit/miss。经过不同分支的读写，会调用底层存储模块来完成磁盘或cache的数据读写。 逻辑处理模块，在代码中对应flashcache_io_callback，它在调度模块通过底层存储模块执行数据读写 操作完成后回调执行，所以说它是“读写后处理模块”，它是采用状态机实现的，根据调度模块中的读写类型进行后续的处理，如读未命中情况下，磁盘读完成后，回调到逻辑处理模块，由它负责将从磁盘读取的数据写回到SSD，或者写未命中情况下，写SSD完成后，回调到逻辑处理模块执行元数据的更新，再有就是对调度模块中读写操作的错误进行处理。 底层存储模块，主要提供了两种方式来完成真实的数据读写，一是由DM提供的dm_io函数，它最终还是通过submit_bio的方式，将由调度模块处理过的bio提交到通用块层，进行转发到真实的设备驱动，完成数据读写；另外，一种方式，kcopyd，是由内核提供的一种底层拷贝函数，主要负责脏块的写回（从SSD到磁盘），会引起元数据的更新。 而后台清理模块，是针对每个set进行数据清理，它会基于两种策略对脏块做回收：（1）set内脏块超过了阈值；（2）脏块超过了设定的空闲时间，即fallow_delay，一般是15分钟，在15分钟没有被操作 则会被优先回收。要注意的是，并没有单独的线程在后台做定期空闲块回收，必须由IO操作触发，如果长时间没有对某set操作，则其中的脏数据很长期保持， 容易危害数据安全。 源代码布局两个工作队列。结合devicemapper代码，特别是dm.c可以知道，在调用flashcache_create工具创建flashcache设备时，会调用flashcache_ctl函数，执行创建工具，它会创建一工作队列_delay_clean，主要负责对整个cache设备的脏块清理，由flashcache_clean_set在特定条件下调用（见代码），通过flashcache_clean_all执行对所有sets的扫描与清理。 另外一个工作队列，_kq_xxx(记不清了)，在flashcache_init中，由flashcache模块加载时执行，通过对5个job链表进行处理，执行元数据的更新与完成处理函数、读磁盘后的SSD写入、以及对等待队列的处理，主要就是负责读写后的处理工作隶属于逻辑处理模块，即“读写后处理 模块”，由磁盘或SSD读写后不同情况下被调度。 调度的时机可以看flashcache_map函数，处理逻辑则主要在函数flashcache_io_callback内部判断，the same block的等待队列是否为空，如果不为空，则同样会调用flashcache_do_handler，执行对等待队列的处理。 数据调度对读，接收到bio，首先，根据bio-&gt;bi_sector，即硬盘的扇区号，得到SSD上的set。其次，在set内查找是否命中，如果命中，则将硬盘的扇区号转换为SSD的扇区号，然后将此bio向SSD提交，进行读取；如果未命中，则首先向硬盘驱动提交bio，从硬盘读数据，读取完成后，由回调函数启动回写SSD操作，将bio的扇区号转换为SSD的=扇区号，然后向SSD驱动程序提交，将硬盘读取的数据写入SSD。对写，同文件系统页缓冲，并不直接写入硬盘，而是写入 SSD，同时，保持一个阀值，一般为20%，在脏块数目达到此数值时，写回磁盘。 模拟实验转载自壬癸甲乙博客 不是每个人都有SSD/PCI-EFlash的硬件，所以这里可以给大家一个构建虚拟混合存储设备的小技巧，这样即使是在自己的笔记本上，也可以轻松的模拟Flashcache的试验环境，而且随便折腾。 首先，我们可以用内存来模拟一个性能很好的Flash设备，当然这有一个缺点，就是主机重启后就啥都没了，不过用于实验测试这应该不是什么大问题。用内存来模拟块设备有两种方法，ramdisk或者tmpfs+loop device。由于ramdisk要调整大小需要修改grub并重启，这里我们用tmpfs来实现。 123456# 限制tmpfs最大不超过10G，避免耗尽内存(测试机器有24G物理内存)$sudo mount tmpfs /dev/shm -t tmpfs -o size=10240m# 创建一个2G的文件，用来模拟2G的flash设备$dd if=/dev/zero of=/dev/shm/ssd.img bs=1024k count=2048# 将文件模拟成块设备$sudo losetup /dev/loop0 /dev/shm/ssd.img tmpfs,临时文件系统，是一种基于内存的文件系统，它和虚拟磁盘ramdisk比较类似像，但不完全相同，和ramdisk一样，tmpfs可以使用RAM，但它也可以使用swap分区来存储。而且传统的ramdisk是个块设备，要用mkfs来格式化它，才能真正地使用它；而tmpfs是一个文件系统，并不是块设备，只是安装它，就可以使用了。tmpfs是最好的基于RAM的文件系统。因为tmpfs是直接建立在VM之上的，您用一个简单的mount命令就可以创建tmpfs文件系统了。 解决了cache设备，还需要有disk持久设备。同样的，可使用普通磁盘上的文件来虚拟成一个loop device。 123# 在普通磁盘的文件系统中创建一个4G的文件，用来模拟4G的disk设备$dd if=/dev/zero of=/u01/jiangfeng/disk.img bs=1024k count=4096$sudo losetup /dev/loop1 /u01/jiangfeng/disk.img 这样我们就有了一个快速的设备/dev/loop0，一个慢速的磁盘设备/dev/loop1，可以开始创建一个Flashcache混合存储设备了。 123456789101112131415161718192021222324252627282930$sudo flashcache_create -p back cachedev /dev/loop0 /dev/loop1cachedev cachedev, ssd_devname /dev/loop0, disk_devname /dev/loop1 cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 8MB of your 48384MB main memory$sudo mkfs.ext3 /dev/mapper/cachedevmke2fs 1.41.12 (17-May-2010)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks262144 inodes, 1048576 blocks52428 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=107374182432 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736Writing inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 28 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override.$sudo mount /dev/mapper/cachedev /u03 Ok，检查一下，就可以开始做一些模拟测试啦。 123456789101112131415161718192021222324252627282930313233343536373839404142434445$sudo dmsetup tablecachedev: 0 8388608 flashcache conf: ssd dev (/dev/loop0), disk dev (/dev/loop1) cache mode(WRITE_BACK) capacity(2038M), associativity(512), data block size(4K) metadata block size(4096b) skip sequential thresh(0K) total blocks(521728), cached blocks(83), cache percent(0) dirty blocks(0), dirty percent(0) nr_queued(0)Size Hist: 4096:84 $sudo dmsetup statuscachedev: 0 8388608 flashcache stats: reads(84), writes(0) read hits(1), read hit percent(1) write hits(0) write hit percent(0) dirty write hits(0) dirty write hit percent(0) replacement(0), write replacement(0) write invalidates(0), read invalidates(0) pending enqueues(0), pending inval(0) metadata dirties(0), metadata cleans(0) metadata batch(0) metadata ssd writes(0) cleanings(0) fallow cleanings(0) no room(0) front merge(0) back merge(0) disk reads(83), disk writes(0) ssd reads(1) ssd writes(83) uncached reads(0), uncached writes(0), uncached IO requeue(0) uncached sequential reads(0), uncached sequential writes(0) pid_adds(0), pid_dels(0), pid_drops(0) pid_expiry(0)$sudo sysctl -a | grep flashcachedev.flashcache.loop0+loop1.io_latency_hist = 0dev.flashcache.loop0+loop1.do_sync = 0dev.flashcache.loop0+loop1.stop_sync = 0dev.flashcache.loop0+loop1.dirty_thresh_pct = 20dev.flashcache.loop0+loop1.max_clean_ios_total = 4dev.flashcache.loop0+loop1.max_clean_ios_set = 2dev.flashcache.loop0+loop1.do_pid_expiry = 0dev.flashcache.loop0+loop1.max_pids = 100dev.flashcache.loop0+loop1.pid_expiry_secs = 60dev.flashcache.loop0+loop1.reclaim_policy = 0dev.flashcache.loop0+loop1.zero_stats = 0dev.flashcache.loop0+loop1.fast_remove = 0dev.flashcache.loop0+loop1.cache_all = 1dev.flashcache.loop0+loop1.fallow_clean_speed = 2dev.flashcache.loop0+loop1.fallow_delay = 900dev.flashcache.loop0+loop1.skip_seq_thresh_kb = 0 sysctl命令被用于在内核运行时动态地修改内核的运行参数，可用的内核参数在目录/proc/sys中。它包含一些TCP/ip堆栈和虚拟内存系统的高级选项， 这可以让有经验的管理员提高引人注目的系统性能。用sysctl可以读取设置超过五百个系统变量。 -n：打印值时不打印关键字； -e：忽略未知关键字错误； -N：仅打印名称； -w：当改变sysctl设置时使用此项； -p：从配置文件“/etc/sysctl.conf”加载内核参数设置； -a：打印当前所有可用的内核参数变量和值； -A：以表格方式打印当前所有可用的内核参数变量和值。 来自: http://man.linuxde.net/sysctl 我是dd一个ssd.img和一个disk.img，然后fio进行测试，效果比非flashcache设备提高3-4倍。","tags":[{"name":"flashcache","slug":"flashcache","permalink":"http://yoursite.com/tags/flashcache/"},{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"块存储","slug":"块存储","permalink":"http://yoursite.com/tags/块存储/"},{"name":"转载","slug":"转载","permalink":"http://yoursite.com/tags/转载/"}]},{"title":"linux设备驱动-1-hello world","date":"2017-05-22T08:00:07.738Z","path":"2017/05/22/linux设备驱动-1-hello world/","text":"内核模块的开发效率更高，而且可以在内核运行时动态加载。由于Linux内核模块是动态加载，所以它也叫可加载内核模块，洋文LKM(Loadable Kernel Module)。Linux内核镜像位于/boot目录下，启动时最先加载，LKM总是在内核启动之后加载。 LKM主要用于：设备驱动、文件系统驱动和系统调用。 下面我来编写一个简单的内核模块，了解一下基本开发流程。 要想编译LKM，我们需要C编译器和Linux内核头文件： 1$ sudo apt-get install build-essential linux-headers-$(uname -r) # Ubuntu/Debian Hello World内核模块代码（hello.c） 123456789101112131415161718192021#include &lt;linux/module.h&gt; /* 模块头文件，必不可少 */#include &lt;linux/kernel.h&gt; /* KERN_INFO在这里 */#include &lt;linux/init.h&gt; /* 使用的宏 */MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;joystorage&quot;);MODULE_DESCRIPTION(&quot;My First LKM!&quot;);static int __init hello_start(void)&#123; printk(KERN_INFO &quot;Hello World\\n&quot;); return 0;&#125;static void __exit hello_end(void)&#123; printk(KERN_INFO &quot;exit hello\\n&quot;);&#125;module_init(hello_start);module_exit(hello_end); module_init定义了模块的入口函数，在模块加载insmoded时执行 module_exit定义了模块的退出函数，在模块卸载rmmoded时执行 创建用于编译内核模块的Makefile： 123456789obj-m = hello.oKERNELDIR ?= /lib/modules/$(shell uname -r)/buildPWD := $(shell pwd)all: $(MAKE) -C $(KERNELDIR) M=$(PWD) modulesclean: $(MAKE) -C $(KERNELDIR) M=$(PWD) clean 输入make编译hello.c 查看模块信息 12345678$ modinfo hello.kofilename: /data/mypro/linux/study/hello.kodescription: My First LKM!author: blog.topspeedsnail.comlicense: GPLsrcversion: 5AAD5F0389D6E1C7146AD2Adepends: vermagic: 4.4.0-21-generic SMP mod_unload modversions 686 加载模块 1$ sudo insmod hello.ko 查看模块 1$ sudo lsmod | grep hello 卸载模块 1$ sudo rmmod hello 查看运行结果 123$sudo dmesg | tail -2[ 1298.186665] Hello World[ 1319.626504] exit hello 备注 我们还需要进一步了解一下modprobe工具。和insmod一样，它也是用来将模块加载到内核中。它和insmod的区别在于，它会考虑要加载的模块是否引用了一些当前内核不存在的符号，如果存在，modprobe会在当前模块中搜索路径中查找定义了这些符号的其他模块，如果找到，它会同时将这些模块装载到内核中。这种情况下使用insmod则会失败，并在日志文件中记录”unresolved symbols”的消息。 如果初始化过程失败，唯一有效的方法是重新引导系统，并注销已经初始化的变量。 错误恢复过程中使用goto语句会比较有效","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"设备驱动","slug":"设备驱动","permalink":"http://yoursite.com/tags/设备驱动/"}]},{"title":"缓存模拟器-zycache","date":"2017-05-12T07:51:18.523Z","path":"2017/05/12/缓存模拟器-zycache/","text":"为了方便实验，最近自己实现了一套缓存模拟器，以SNIA网站上提供的TRACE为输入，分别实现了FIFO,LRU,LFU,OPT,ARC等经典的缓存算法，整个模拟器通过C++实现，利用面向对象思想。代码见github 运行的大致结果如下： 下面贴出来zy-cache的LRU和OPT算法的实现，其中OPT根据自己的理解分别实现了时间最优算法optt和空间最优算法opts，分别试用于电脑内存空间充足和不足的场景使用。 LRU 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156class lru : public algorithm&#123;private://缓存基本信息 struct list_entry *lru_hot; //lru的hot端 struct list_entry *lru_cold; //lru的cold端 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map; //缓存索引public: //缓存基本操作 //初始化 lru(int cache_s, int block_s, int wt) &#123; strcpy(name,&quot;lru&quot;); outfile = NULL; outfile = open_file(&quot;lru.out&quot;); cache_size = cache_s; block_size = block_s; write_type = wt; block_num = cache_size / (512*block_size) ; hit_num = 0; total_num = 0; read_num = 0; ssd_write = 0; lru_cold = lru_hot = NULL; &#125; ~lru() &#123; //销毁lru链表 while(lru_hot != NULL) &#123; struct list_entry *le1 = lru_hot; lru_hot = le1-&gt;next; free(le1); le1 = NULL; &#125; lru_hot = lru_cold = NULL; //销毁map cache_map.clear(); close_file(outfile); outfile = NULL; &#125; void map_operation(u_int64_t key, int io_type,struct trace_inf *ti) &#123; unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got = cache_map.find(key); if(got == cache_map.end()) &#123;//未命中 //cout &lt;&lt; cache_map.size() &lt;&lt; endl; if(cache_map.size() == block_num) &#123;//缓存满，需要替换 out_put(ti,lru_cold-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 u_int64_t id_del = lru_cold-&gt;block_id; cache_map.erase(id_del); lru_cold-&gt;next = lru_hot; lru_hot-&gt;pre = lru_cold; lru_hot = lru_cold; lru_cold = lru_cold-&gt;pre; lru_cold-&gt;next = lru_hot-&gt;pre = NULL; lru_hot-&gt;block_id = key; lru_hot-&gt;access_cnt = 1; lru_hot-&gt;pre_access = total_num; cache_map[key] = lru_hot; &#125; else &#123; struct list_entry * le = (struct list_entry *)malloc(sizeof(list_entry)); le-&gt;lbn = cache_map.size(); if(lru_hot == NULL) &#123; le-&gt;next = le-&gt;pre = NULL; le-&gt;block_id = key; le-&gt;access_cnt = 1; le-&gt;pre_access = total_num; lru_hot = lru_cold = le; cache_map[key] = le; out_put(ti,le-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; le-&gt;pre = NULL; le-&gt;block_id = key; le-&gt;access_cnt = 1; le-&gt;pre_access = total_num; le-&gt;next = lru_hot; lru_hot-&gt;pre = le; lru_hot = le; cache_map[key] = le; out_put(ti,le-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 &#125; le = NULL; &#125; &#125; else &#123;//命中 struct list_entry * le = got-&gt;second; if(lru_hot != le) &#123; le-&gt;pre-&gt;next = le-&gt;next; if(le-&gt;next != NULL)&#123; le-&gt;next-&gt;pre = le-&gt;pre; &#125;else&#123; lru_cold = le-&gt;pre; lru_cold-&gt;next = NULL; &#125; le-&gt;next = lru_hot; lru_hot-&gt;pre = le; le-&gt;pre = NULL; lru_hot = le; &#125; le-&gt;access_cnt++; le-&gt;pre_access = total_num; hit_num++; if(io_type == 1)&#123; //写与读不同的地方 out_put(ti,le-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; out_put(ti,le-&gt;lbn,&quot;Read&quot;);//模拟替换时SSD的写 &#125; le = NULL; &#125; //lru链表测试#if DEBUG struct list_entry *le1 = lru_hot; while(le1 != NULL)&#123; cout &lt;&lt; le1-&gt;block_id &lt;&lt; &quot;\\t&quot;; le1 = le1-&gt;next; &#125; cout &lt;&lt; endl;#endif &#125;&#125;; opt 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399#include &lt;list&gt;struct next_acc&#123; u_int64_t num; struct next_acc *next;&#125;;struct map_entry&#123; int acc_num; struct next_acc *head; struct next_acc *tail;&#125;;class optt : public algorithm&#123; //opt时间最优算法 //且处理时间为log(n)级别 //得到opt绝对最优解，内存空间充足情况下使用private://缓存基本信息 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map; //缓存索引 struct list_entry cache_entry[block_num_conf]; unordered_map&lt;u_int64_t,struct map_entry *&gt; io_map; //预处理map u_int64_t pre_cnt; u_int64_t total_rec;public: //缓存基本操作 //初始化 optt(int cache_s, int block_s, int wt) &#123; outfile = NULL; outfile = open_file(&quot;opt.out&quot;); strcpy(name,&quot;opt&quot;); cache_size = cache_s; block_size = block_s; write_type = wt; block_num = cache_size / (512*block_size) ; hit_num = 0; total_num = 0; read_num = 0; ssd_write = 0; pre_cnt = 0; total_rec = 1.8446744*1e19; //memset(cache_entry,0xff,block_num_conf); &#125; ~optt() &#123; //销毁map io_map.clear(); cache_map.clear(); close_file(outfile); outfile = NULL; &#125; void map_operation(u_int64_t key, int io_type,struct trace_inf *ti) &#123; unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got = cache_map.find(key); if(got == cache_map.end()) &#123;//未命中 int map_size = cache_map.size(); //cout &lt;&lt; cache_map.size() &lt;&lt; endl; if(map_size == block_num) &#123;//缓存满，需要替换 int victim = find_victim(); u_int64_t id_del = cache_entry[victim].block_id; cache_map.erase(id_del); cache_entry[victim].block_id = key; cache_entry[victim].access_cnt = 1; cache_entry[victim].pre_access = total_num; cache_entry[victim].next_access = find_next(key); cache_map[key] = &amp;cache_entry[victim]; out_put(ti,victim,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; cache_entry[map_size].block_id = key; cache_entry[map_size].access_cnt = 1; cache_entry[map_size].pre_access = total_num; cache_entry[map_size].next_access = find_next(key); cache_entry[map_size].lbn = map_size; cache_map[key] = &amp;cache_entry[map_size]; out_put(ti,map_size,&quot;Write&quot;);//模拟替换时SSD的写 &#125; &#125; else &#123;//命中 struct list_entry * le = got-&gt;second; le-&gt;access_cnt++; le-&gt;pre_access = total_num; le-&gt;next_access = find_next(key); hit_num++; if(io_type == 1)&#123; //写与读不同的地方 out_put(ti,le-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; out_put(ti,le-&gt;lbn,&quot;Read&quot;);//模拟替换时SSD的写 &#125; le = NULL; &#125; //lru链表测试#if DEBUG int cs = cache_map.size(); for(int i=0 ; i &lt; cs ; i++) &#123; cout &lt;&lt; cache_entry[i].block_id &lt;&lt; &quot;\\t&quot;; &#125; cout &lt;&lt; endl;#endif &#125; int find_victim() &#123; int cs = cache_map.size(); int result = 0; int max_next = cache_entry[0].next_access; //cout &lt;&lt; cs &lt;&lt; &quot;\\t&quot;; for(int i=0 ; i &lt; cs ; i++) &#123; //cout &lt;&lt; cache_entry[i].block_id &lt;&lt; &quot;\\t&quot;; if(cache_entry[i].next_access &gt; max_next) &#123; max_next = cache_entry[i].next_access; result = i; &#125; &#125; return result; &#125; u_int64_t find_next(u_int64_t block_id) &#123; unordered_map&lt;u_int64_t,struct map_entry*&gt;::iterator it = io_map.find(block_id); struct next_acc *nt = NULL; struct map_entry *me = NULL; if(it != io_map.end()) &#123; me = it-&gt;second; if(me-&gt;head == NULL) &#123; return total_rec + 1; &#125; else&#123; nt = me-&gt;head; me-&gt;head = nt-&gt;next; delete nt; if(me-&gt;head != NULL) &#123; return me-&gt;head-&gt;num; &#125; else &#123; return total_rec + 1; &#125; &#125; &#125; else &#123; cout &lt;&lt; &quot;Severe Error!!!&quot; &lt;&lt; endl; exit(0); &#125; &#125; //初始化io_list void init_io_list(struct trace_inf *ti) &#123; struct bio *head = div_block(ti); while(head != NULL) &#123; pre_cnt++; unordered_map&lt;u_int64_t,struct map_entry*&gt;::iterator it = io_map.find(head-&gt;block_id); struct next_acc *nt = NULL; struct map_entry *me = NULL; if(it == io_map.end()) &#123; me = (struct map_entry*)malloc(sizeof(struct map_entry)); nt = (struct next_acc *)malloc(sizeof(struct next_acc)); nt-&gt;num = pre_cnt; nt-&gt;next = NULL; me-&gt;head = me-&gt;tail = nt; me-&gt;acc_num = 1; io_map[head-&gt;block_id] = me; &#125;else&#123; nt = (struct next_acc *)malloc(sizeof(struct next_acc)); nt-&gt;num = pre_cnt; nt-&gt;next = NULL; me = it-&gt;second; me-&gt;tail-&gt;next = nt; me-&gt;tail = nt; me-&gt;acc_num++; &#125; head = head-&gt;next; &#125; &#125; void show_io_list()&#123; struct next_acc *nt = NULL; struct map_entry *me = NULL; for ( unordered_map&lt;u_int64_t,struct map_entry*&gt;::iterator it=io_map.begin(); it != io_map.end(); ++it) &#123; me = it-&gt;second; nt = me-&gt;head; cout &lt;&lt; it-&gt;first &lt;&lt; &quot;\\t&quot; &lt;&lt; me-&gt;acc_num &lt;&lt; &quot;\\t&quot;; while(nt != NULL) &#123; cout &lt;&lt; nt-&gt;num &lt;&lt; &quot;\\t&quot;; nt = nt-&gt;next; &#125; cout &lt;&lt; endl; &#125; &#125;&#125;;class opts : public algorithm&#123; //opt空间最优算法，可以通过OPT_DEGREE调整算法级别，得到的知识opt的一个次优解 //OPT_DEGREE为1-10的整数，在conf.h中进行设置，数越大越接近理论最优，但是运行时间也越长 //内存空间有限的情况下使用 //时间复杂度为n*nprivate://缓存基本信息 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map; //缓存索引 struct list_entry cache_entry[block_num_conf]; list&lt;u_int64_t&gt; io_list;public: //缓存基本操作 //初始化 opts(int cache_s, int block_s, int wt) &#123; strcpy(name,&quot;opt&quot;); cache_size = cache_s; block_size = block_s; write_type = wt; block_num = cache_size / (512*block_size) ; hit_num = 0; total_num = 0; read_num = 0; ssd_write = 0; //memset(cache_entry,0xff,block_num_conf); &#125; ~opts() &#123; //销毁map cache_map.clear(); &#125; void map_operation(u_int64_t key, int io_type,struct trace_inf *ti) &#123; unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got = cache_map.find(key); if(got == cache_map.end()) &#123;//未命中 int map_size = cache_map.size(); //cout &lt;&lt; cache_map.size() &lt;&lt; endl; if(map_size == block_num) &#123;//缓存满，需要替换 int victim = find_victim(); u_int64_t id_del = cache_entry[victim].block_id; cache_map.erase(id_del); cache_entry[victim].block_id = key; cache_entry[victim].access_cnt = 1; cache_entry[victim].pre_access = total_num; cache_entry[victim].next_access = find_next(key); cache_map[key] = &amp;cache_entry[victim]; out_put(ti,victim,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; cache_entry[map_size].block_id = key; cache_entry[map_size].access_cnt = 1; cache_entry[map_size].pre_access = total_num; cache_entry[map_size].next_access = find_next(key); cache_entry[map_size].lbn = map_size; cache_map[key] = &amp;cache_entry[map_size]; out_put(ti,map_size,&quot;Write&quot;);//模拟替换时SSD的写 &#125; &#125; else &#123;//命中 struct list_entry * le = got-&gt;second; le-&gt;access_cnt++; le-&gt;pre_access = total_num; le-&gt;next_access = find_next(key); hit_num++; if(io_type == 1)&#123; //写与读不同的地方 out_put(ti,le-&gt;lbn,&quot;Write&quot;);//模拟替换时SSD的写 &#125; else &#123; out_put(ti,le-&gt;lbn,&quot;Read&quot;);//模拟替换时SSD的写 &#125; le = NULL; &#125; //lru链表测试#if DEBUG int cs = cache_map.size(); for(int i=0 ; i &lt; cs ; i++) &#123; cout &lt;&lt; cache_entry[i].block_id &lt;&lt; &quot;\\t&quot;; &#125; cout &lt;&lt; endl;#endif &#125; int find_victim() &#123; int cs = cache_map.size(); int result = 0; int max_next = cache_entry[0].next_access; //cout &lt;&lt; cs &lt;&lt; &quot;\\t&quot;; for(int i=0 ; i &lt; cs ; i++) &#123; //cout &lt;&lt; cache_entry[i].block_id &lt;&lt; &quot;\\t&quot;; if(cache_entry[i].next_access &gt; max_next) &#123; max_next = cache_entry[i].next_access; result = i; &#125; &#125; return result; &#125; u_int64_t find_next(u_int64_t block_id) &#123; u_int64_t result = total_num; int cnt = 0; for (list&lt;u_int64_t&gt;::iterator it=io_list.begin(); it != io_list.end(); ++it) &#123; cnt++; if(*it == block_id) &#123; break; &#125; if(cnt &gt; OPT_DEGREE * block_num_conf)&#123; cnt = io_list.size()*2; break; &#125; &#125; return (result + cnt - 1); &#125; //初始化io_list void init_io_list(struct trace_inf *ti) &#123; struct bio *head = div_block(ti); while(head != NULL) &#123; io_list.push_back(head-&gt;block_id); head = head-&gt;next; &#125; &#125; void show_io_list()&#123; for (list&lt;u_int64_t&gt;::iterator it=io_list.begin(); it != io_list.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; &quot;\\t&quot;; &#125; cout &lt;&lt; endl; &#125;&#125;; ARC 其中arc算法逻辑如下图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381class arc : public algorithm&#123;private://缓存基本信息 struct list_entry *hot_t1; //fifo的hot端 struct list_entry *cold_t1; //fifo的cold端 struct list_entry *hot_b1; //fifo的hot端 struct list_entry *cold_b1; //fifo的cold端 struct list_entry *hot_t2; //fifo的hot端 struct list_entry *cold_t2; //fifo的cold端 struct list_entry *hot_b2; //fifo的hot端 struct list_entry *cold_b2; //fifo的cold端 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map_t1; //缓存索引 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map_b1; //缓存索引 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map_t2; //缓存索引 unordered_map&lt;u_int64_t,struct list_entry *&gt; cache_map_b2; //缓存索引 int p; int del; int c;public: //缓存基本操作 //初始化 arc(int cache_s, int block_s, int wt) &#123; outfile = NULL; outfile = open_file(&quot;arc.out&quot;); strcpy(name,&quot;arc&quot;); cache_size = cache_s; block_size = block_s; write_type = wt; block_num = cache_size / (512*block_size) ; hit_num = 0; total_num = 0; read_num = 0; ssd_write = 0; hot_t1 = cold_t1 = NULL; hot_b1 = cold_b1 = NULL; hot_t2 = cold_t2 = NULL; hot_b2 = cold_b2 = NULL; p = 0 ; c = block_num; &#125; ~arc() &#123; close_file(outfile); outfile = NULL; //销毁lru链表 struct list_entry *le1; while(hot_t1 != NULL) &#123; le1 = hot_t1; hot_t1 = le1-&gt;next; free(le1); le1 = NULL; &#125; while(hot_t2 != NULL) &#123; le1 = hot_t2; hot_t2 = le1-&gt;next; free(le1); le1 = NULL; &#125; while(hot_b1 != NULL) &#123; le1 = hot_b1; hot_b1 = le1-&gt;next; free(le1); le1 = NULL; &#125; while(hot_b2 != NULL) &#123; le1 = hot_b2; hot_b2 = le1-&gt;next; free(le1); le1 = NULL; &#125; hot_t1 = cold_t1 = hot_t2 = cold_t2 = hot_b1 = hot_b2 = cold_b1 = cold_b2 = NULL; //销毁map cache_map_t1.clear(); cache_map_t2.clear(); cache_map_b1.clear(); cache_map_b2.clear(); &#125; void map_operation(u_int64_t key, int io_type,struct trace_inf *ti) &#123; struct list_entry *le; int t1 = cache_map_t1.size(); int t2 = cache_map_t2.size(); int b1 = cache_map_b1.size(); int b2 = cache_map_b2.size(); unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got = cache_map_t1.find(key); if(got == cache_map_t1.end()) &#123;//t1未命中 unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got2 = cache_map_t2.find(key); if(got2 == cache_map_t2.end()) &#123;//t2未命中 unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got3 = cache_map_b1.find(key); if(got3 == cache_map_b1.end()) &#123;//b1中未命中 unordered_map&lt;u_int64_t,struct list_entry *&gt;::iterator got4 = cache_map_b2.find(key); if(got4 == cache_map_b2.end()) &#123;//都未命中---case4 if(t1+b1 == c) &#123; if(t1 &lt; c) &#123;//删除 b1的lru端 del_cold(&amp;cache_map_b1,&amp;hot_b1,&amp;cold_b1,&amp;le); if(le != NULL) &#123; free(le); &#125; replace(p,4); &#125; else &#123;//b1是空的,删除t1的lru端 del_cold(&amp;cache_map_t1,&amp;hot_t1,&amp;cold_t1,&amp;le); if(le != NULL) &#123; free(le); &#125; &#125; &#125; else if( t1 + b1 &lt; c) &#123;//t1+b1 &lt; c if(t1+t2+b1+b2 &gt;= c) &#123; if(t1+t2+b1+b2 == 2*c) &#123; //删除b2的lru端 del_cold(&amp;cache_map_b2,&amp;hot_b2,&amp;cold_b2,&amp;le); if(le!=NULL) &#123; free(le); &#125; &#125; replace(p,4); &#125; &#125; struct list_entry *lie = (struct list_entry *)malloc(sizeof(struct list_entry)); lie-&gt;pre = NULL; lie-&gt;next = hot_t1; lie-&gt;block_id = key; lie-&gt;access_cnt = 1; lie-&gt;pre_access = total_num; add_hot(&amp;cache_map_t1,&amp;hot_t1,&amp;cold_t1,lie); ssd_write++; &#125; else &#123;//缓存未命中，但b2命中---case3 le = got4-&gt;second; if(b2 &gt;= b1) &#123; del = 1; &#125; else &#123; del = b1 / b2; &#125; if(p-del &gt; 0) &#123; p -= del; &#125; else &#123; p = 0; &#125; replace(p,3); del_poi(&amp;cache_map_b2,&amp;hot_b2,&amp;cold_b2,le); add_hot(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,le); ssd_write++; &#125; &#125; else &#123;//缓存未命中，但b1命中---case2 le = got3-&gt;second; if(b1 &gt;= b2) &#123; del = 1; &#125; else &#123; del = b2 / b1; &#125; if(p+del &lt; c) &#123; p += del; &#125; else &#123; p = c; &#125; replace(p,2); del_poi(&amp;cache_map_b1,&amp;hot_b1,&amp;cold_b1,le); add_hot(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,le); ssd_write++; &#125; &#125; else &#123;//t2命中---case1 le = got2-&gt;second; del_poi(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,le); add_hot(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,le); hit_num++; if(io_type == 1)&#123; //写与读不同的地方 ssd_write++; &#125; &#125; &#125; else &#123;//t1命中---case1 le = got-&gt;second; del_poi(&amp;cache_map_t1,&amp;hot_t1,&amp;cold_t1,le); add_hot(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,le); hit_num++; if(io_type == 1)&#123; //写与读不同的地方 ssd_write++; &#125; &#125; //lru链表测试#if DEBUG display();#endif &#125; void replace(int pp, int flag) &#123; struct list_entry *le; int tmp = cache_map_t1.size(); if((tmp != 0)&amp;&amp;((tmp &gt; pp) || (flag == 3 &amp;&amp; tmp == p))) &#123; del_cold(&amp;cache_map_t1,&amp;hot_t1,&amp;cold_t1,&amp;le); add_hot(&amp;cache_map_b1,&amp;hot_b1,&amp;cold_b1,le); &#125; else &#123; del_cold(&amp;cache_map_t2,&amp;hot_t2,&amp;cold_t2,&amp;le); add_hot(&amp;cache_map_b2,&amp;hot_b2,&amp;cold_b2,le); &#125; &#125; void del_cold(unordered_map&lt;u_int64_t,struct list_entry *&gt; *cache_map, struct list_entry **hot, struct list_entry **cold,struct list_entry **le) &#123; *le = *cold; if(*cold != NULL) &#123; (*cache_map).erase((*le)-&gt;block_id); if((*le)-&gt;pre != NULL)&#123; (*le)-&gt;pre-&gt;next = (*le)-&gt;next; *cold = (*le)-&gt;pre; if(*cold != NULL) &#123; (*cold)-&gt;next = NULL; &#125; &#125;else&#123; *hot = *cold = NULL; &#125; &#125; &#125; void del_poi(unordered_map&lt;u_int64_t,struct list_entry *&gt; *cache_map, struct list_entry **hot, struct list_entry **cold, struct list_entry *le) &#123; if(le != NULL) &#123; (*cache_map).erase(le-&gt;block_id); if(le-&gt;pre != NULL)&#123; le-&gt;pre-&gt;next = le-&gt;next; &#125;else&#123; *hot = le-&gt;next; &#125; if(le-&gt;next != NULL)&#123; le-&gt;next-&gt;pre = le-&gt;pre; &#125;else&#123; *cold = le-&gt;pre; &#125; if(*hot != NULL) &#123; (*hot)-&gt;pre = NULL; &#125; if(*cold != NULL) &#123; (*cold)-&gt;next = NULL; &#125; &#125; &#125; void add_hot(unordered_map&lt;u_int64_t,struct list_entry *&gt; *cache_map, struct list_entry ** hot, struct list_entry ** cold,struct list_entry *le) &#123; if(le != NULL) &#123; (*cache_map)[le-&gt;block_id] = le; le-&gt;next = *hot; if(*cold == NULL) &#123; *cold = le; &#125; if(*hot != NULL) &#123; (*hot)-&gt;pre = le; &#125; *hot = le; (*hot)-&gt;pre = NULL; &#125; &#125; void display() &#123; struct list_entry *le1; le1 = hot_b2; cout &lt;&lt; &quot;b2 :&quot; &lt;&lt; &quot;\\t&quot; ; while(le1 != NULL) &#123; cout &lt;&lt; le1-&gt;block_id &lt;&lt; &quot;\\t&quot;; le1 = le1-&gt;next; &#125; le1 = hot_t2; cout &lt;&lt; &quot;t2 :&quot; &lt;&lt; &quot;\\t&quot; ; while(le1 != NULL) &#123; cout &lt;&lt; le1-&gt;block_id &lt;&lt; &quot;\\t&quot;; le1 = le1-&gt;next; &#125; le1 = hot_t1; cout &lt;&lt; &quot;t1 :&quot; &lt;&lt; &quot;\\t&quot; ; while(le1 != NULL) &#123; cout &lt;&lt; le1-&gt;block_id &lt;&lt; &quot;\\t&quot;; le1 = le1-&gt;next; &#125; le1 = hot_b1; cout &lt;&lt; &quot;b1 :&quot; &lt;&lt; &quot;\\t&quot; ; while(le1 != NULL) &#123; cout &lt;&lt; le1-&gt;block_id &lt;&lt; &quot;\\t&quot;; le1 = le1-&gt;next; &#125; cout &lt;&lt; cache_map_b2.size() &lt;&lt;&quot; &quot;&lt;&lt;cache_map_t2.size() &lt;&lt; &quot; &quot;&lt;&lt;cache_map_t1.size()&lt;&lt;&quot; &quot;&lt;&lt;cache_map_b1.size(); cout &lt;&lt; endl; &#125;&#125;;","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"缓存","slug":"缓存","permalink":"http://yoursite.com/tags/缓存/"},{"name":"zycache","slug":"zycache","permalink":"http://yoursite.com/tags/zycache/"}]},{"title":"般若波罗蜜多心经","date":"2017-04-28T05:37:21.005Z","path":"2017/04/28/般若波罗蜜多心经/","text":"原文123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475bō rě bō luó mì duō xīn jīng般若波罗蜜多心经guān zì zài pú sà , xíng shēn bō rě bō luó mì duō shí，zhào jiàn wǔ yùn jiē kōng, dù yī qiē kǔ è。观自在菩萨，行深般若波罗蜜多时，照见五蕴皆空，度一切苦厄。shè lì zǐ, sè bù yì kōng ，kōng bù yì sè , sè jí shì kōng , kōng jí shì sè。舍利子，色不异空，空不异色，色即是空，空即是色。shòu xiǎng xíng shí，yì fù rú shì。受想行识，亦复如是。shè lì zǐ, shì zhū fǎ kōng xiāng,舍利子，是诸法空相，bù shēng bù miè, bù gòu bù jìng, bù zēng bù jiǎn ,不生不灭，不垢不净，不增不减，shì gù kōng zhōng wú sè, wú shòu xiǎng xíng shí,是故空中无色，无受想行识，wú yǎn ěr bí shé shēn yì, wú sè shēng xiāng wèi chù fǎ, wú yǎn jiè,无眼耳鼻舌身意，无色声香味触法，无眼界，nǎi zhì wú yì shí jiè, wú wú míng , yì wú wú míng jìn,乃至无意识界，无无明，亦无无明尽，nǎi zhì wú lǎo sǐ, yì wú lǎo sǐ jìn。乃至无老死，亦无老死尽。wú kǔ jí miè dào, wú zhì yì wú dé, yǐ wú suǒ dé gù。无苦集灭道，无智亦无得，以无所得故。pú tí sà duǒ yī bō rě bō luó mì duō gù xīn wú guà ài。菩提萨埵，依般若波罗蜜多故，心无挂碍。wú guà ài gù, wú yǒu kǒng bù, yuǎn lí diān dǎo mèng xiǎng, jiū jìng niè pán。无挂碍故，无有恐怖，远离颠倒梦想，究竟涅槃。sān shì zhū fó, yī bō rě bō luó mì duō gù, dé ā nuò duō luó sān miǎo sān pú tí。三世诸佛，依般若波罗蜜多故，得阿耨多罗三藐三菩提。gù zhī bō rě bō luó mì duō, shì dà shén zhòu, shì dà míng zhòu,故知般若波罗蜜多，是大神咒，是大明咒，shì wú shàng zhòu, shì wú děng děng zhòu。néng chú yī qiē kǔ, zhēn shí bù xū。是无上咒，是无等等咒。能除一切苦，真实不虚。gù shuō bō rě bō luó mì duō zhòu。故说般若波罗蜜多咒。jí shuō zhòu yuē：即说咒曰：jiē dì jiē dì, bō luó jiē dì, bō luó sēng jiē dì, pú tí sà pó hē。揭谛揭谛，波罗揭谛，波罗僧揭谛，菩提萨婆诃。 经名整段话的概略意思是“透过心量广大的通达智慧，而超脱世俗困苦的根本途径”。 “摩诃”：无边无际的大、心量广大[1]。比喻宇宙万物大自然之间的规律与特质，约略相当于中国传统文化指称的道与广义的命。 “般若”为梵语音译，指通达妙智慧; “波罗”为梵语音译，指到彼岸(不生不灭、不垢不净)，有解脱挂碍的意思; “蜜多”为梵语音译，意为无极。可联想比如蜜蜂采花酿蜜，能融合众多不同来源成分而归纳为一。 “心”：根本、核心、精髓。一方面表示内容所探讨的主体重心，另一方面也表示全篇内容的重要性。 “经”：字义是线、路、径，引申为经典。代表前人走过的路途、独特而深入的经历或见解，借口述语言或文字记载来传承后世，以供人们做为参考指引。 经文心经经文以“观自在菩萨”开头，以“菩提萨婆诃”结尾(萨婆诃本为祝颂语，亦有观自在的意思，与经文开头相呼应)。 “舍利子(舍利弗)”是心经全文关键字词之一。 起源释迦牟尼佛初转法轮先说四圣谛，即苦集灭道。灭谛中提及涅槃，为了阐释涅槃的内涵及意义，佛陀更深入说明空性之理。第二转无相法轮，借由对空性的认知，证明烦恼是可以断除的，从色法到一切遍智空，一切法皆无自性。有些论师不了解甚深空性，佛陀便对无自性再做解释，第三转善分别法轮的《解深密经》、《如来藏经》、慈氏菩萨的《相续本母经》，详细说明心的体性是惟明惟知，具有原始自然之光明。 《般若经》及诸部般若，为佛陀在二转无相法轮时所宣说，乃大乘佛法中之深法。在藏传的经论中经常提到：“佛说八万四千法门中，般若法门最为殊胜。” 《般若经》的内涵以空性为主，透过对空性的了解能断除烦恼障而得到小乘的涅槃，即声闻及独觉的菩提果位;也能够透过对空性的认识，再加上福德资粮的圆满，能彻底断除所知障而获得大乘的涅槃，即无上的菩提果位。因为解了空性贯穿三乘，故解空被称为三乘之母，诠释它的般若经亦称为母般若。《般若波罗密多心经》即是《大般若经》的心髓，全部般若的精义皆设于此经[2]，故名为《心经》。 佛说《心经》的缘起，是在灵鹫山中部，为诸菩萨声闻弟子所围绕，当时观自在菩萨正在观修般若波罗密多、专注思惟观修而照见五蕴皆自性空。心经主要内涵是舍利子与观自在菩萨有关空性的问答。佛出定后，认可菩萨所说，欢喜赞叹。 心经内涵可分两种，显义与隐义。显义为观空正见，为龙树菩萨的《中论》所阐释。隐义则为现观道次第，间接显示空性所依的有法，为弥勒菩萨所造的《现观庄严论》所诠释。 有学者认为《心经》经文结构之来源，大部分出于《大般若经》第二会观照品第三之二，即《大品般若》习应品第三)。“般若波罗密多是大神咒……”一段，出于《大般若经》第二会功德品第三十二，即《大品般若》劝持品第三十四。咒文则出于《佛说陀罗尼集经》第三卷，般若大心陀罗尼第十六。故《心经》是出自《般若经》的精髓，附加密咒真言，同时奉请观自在菩萨为其说法主，才完成现今《心经》组织的型态[3]。 《大般若经》中所开示之般若法门是专为已发菩提心之众菩萨们所宣说的。其最重要的观念在于以空性智慧觉悟诸法实相(即一切外在事物的名相，皆是自心的虚妄分别而已)，既不体证、进入涅槃而自愿生生世世轮回生死救度众生，其行为看似有违一般所认知的脱离轮回观念，而实际上这才是《大般若经》开悟菩萨的主旨所在。因为以慈悲喜舍之心平等救护一切众生才是真菩萨行，而自己逃离生死轮回却弃众生于不顾则有违菩萨自度度他之初衷誓愿。 在《大般若经》中数度出现 “菩萨摩诃萨普为利乐诸有情故，求趣无上正等菩提”与 “观诸法皆空，不舍一切有情”字句。此即表示若离开对众生的慈悲济度，则一切修行的意义则大打折扣，不能最终成就无上菩提正果。 白话解释12345678910111213141516171819202122232425262728293031323334353637383940414243444546观自在菩萨（般若智慧已经达到自在境界的菩萨）行深般若波罗蜜多时（当他修行般若智慧达到波罗蜜多觉悟境界的时候）照见五蕴皆空（洞见色、受、想、行、识五蕴乃是人类虚空的妄想）度一切苦厄（所以菩萨要为众生解脱一切执着于生死烦恼的苦厄）舍利子（智慧第一的舍利子啊）色不异空（你所看见的物质世界其实是你的精神世界）空不异色（你的精神世界也就是你以为的物质世界）色即是空（物质世界就是精神世界）空即是色（精神世界就是物质世界）受想行识亦复如是（人类所谓的感受、思想、行为和认识也是如此）舍利子（智慧第一的舍利子啊）是诸法空相（其实一切法都不是法，只是人类虚空的精神幻觉）不生不灭（真实的世界不会产生，也不会灭亡）不垢不净（不会被尘埃沾污，也不需要去洁净）不增不减（任何东西都不会增加，也不会减少）是故空中无色（在真实世界里并没有物质这一概念）无受想行识（自然也就不存在人类对物质世界的感受、思想、行为和认识）无眼耳鼻舌身意（眼、耳、鼻、舌、身、意这六种感官对于真实世界没有任何意义）无色声香味触法（自然也就不存在所谓的颜色、声音、香气、味道、感觉和概念）无眼界乃至无意识界（你眼睛所看到的一切都是假象，你的意识也全部都是错觉）无无明亦无无明尽（没有前世愚昧的事情，也没有后世报应的所谓十二因缘）乃至无老死（就连生老病死也是胡说）亦无老死尽（更没有生死轮回的道理）无苦集灭道（没有生死烦恼，没有贪婪和恐惧，也没有所谓的真理）无智亦无得（既没有智慧，也得不到任何知识）以无所得故（这才是超越了人类精神的惟一真实的世界）菩提萨埵（大慈大悲普度众生的观自在菩萨）依般若波罗蜜多故（依靠般若智慧抵达了波罗蜜多觉悟的彼岸）心无罣碍，无罣碍故（心中没有任何牵挂和妨碍，正是因为没有受到人类精神影响的缘故）无有恐怖，远离颠倒梦想（没有任何恐怖，远离那些违背自然的思想）究竟涅盘（所以菩萨观得自在，消除了一切烦恼）三世诸佛（过去、现在和将来三世佛以及一切佛）依般若波罗蜜多故（都是依靠般若智慧达到波罗蜜多觉悟的境界）得阿耨多罗三藐三菩提（得到阿耨多罗无上三藐正等三菩提正觉成佛的境界）故知般若波罗蜜多是大神咒（所以般若智慧波罗蜜多觉悟是不可思议的咒语）是大明咒（是普照一切的咒语）是无上咒（是最最伟大的咒语）是无等等咒（是超度一切的咒语）能除一切苦（能够解除人生的一切生死烦恼和苦厄）真实不虚（佛无妄语，自然真实）故说般若波罗蜜多咒（所以这是众生修行般若智慧抵达波罗蜜多觉悟的密咒）即说咒曰（咒语曰）揭谛揭谛（去吧去吧）波罗揭谛（走过所有的道路）波罗僧揭谛（一起去向人生的彼岸）菩提萨婆诃（欢呼觉悟吧） 转载自：心经全文 般若波罗蜜多心经(全文注音、拼音)","tags":[{"name":"转载","slug":"转载","permalink":"http://yoursite.com/tags/转载/"},{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"一叶孤舟，自强不息","date":"2017-04-20T13:26:34.271Z","path":"2017/04/20/一叶孤舟，自强不息/","text":"这一年，感觉是二十多年来成长最多的一年，并不是提升了多少专业技能，看了多少论文，而是看透了很多事，认识到了自身的诸多不足，这一点得感谢老板的谆谆教诲。 研一的生活主要是上课，老板并没有在科研上过多的push，也没有见过几次面。但仅有的几次交流却让我认识到了自身的很多缺点，而这些是我二十多年都没有看到的。这也是为什么佩服老板了，看的透彻，总是一语道破，而且没有一句大话、空话！ 总结有以下几点： 责任心不够强，不够自立！ 做事不够高效专注，停留表面，较为浮躁！ 交流不能抓住重点！ 我认识到这些缺点对我来说是致命的，足以掩盖我所有的优点！ 我仔细分析了为什么会这样，找到了问题的根结所在，并且做了如下的努力： 经济独立，学会独处家中老幺，从小在家娇生惯养，让我过分依赖父母，这种依赖不光是经济上的，更是精神上的，精神上的不自立是最可怕的，这也是为什么会责任心不强，遇事不先考虑自己处理，而是首先寻求他人援助，因此也就显得没有主见，永远像个奶油小生！ 意识到这一点，我做了两件事！ 经济独立 从上学期开始，就不再向家中要一分钱了，自己养活自己，不管钱多钱少，艰苦与否，自己学会理财，不向家中寻求帮助！ 独处 这种独处不是自闭，不社交，相反，社交生活不能少，只是遇事少找人倾述，不要对任何人产生依赖，自己学会分析考虑问题，少发牢骚，多干实事！ 坚持，自律读研以后我越发感觉到自己的浮躁，不能静下心来做一件事，总是东一榔头，西一棒槌！这应该是小学就有的恶习，甚至高考还在上面栽了跟头。高考的痛楚让我在本科学习一直目标明确，最后也成功保研。但是近来，特别是大四保研以后，浮躁的恶习又有了反弹。年后，发现这个苗头后，防微杜渐，便做了三件事！ 坚持锻炼 每天晚上只要天气好，就坚持去操场跑五圈，跑的不多，贵在坚持！ 坚持背单词 其实这是很多人都坚持不下来的事，但是至今已经坚持了两个多月，且英语水平提高显著！ 自律，自我管理，坚持早起 实验室没有打卡，因此很多人都较为懒散。年后，我开始了自我管理，制定时间表，自觉到实验室科研，早睡早起。现在发现自己不仅精神十足，效率高，并且心情也变好了。 多做presentation，同时培养多门兴趣爱好 Presentation 研讨课上主动做presentation，读完论文，或者学了某项技术后花大量时间总结，关键是总结时要理清思路，培养自己的逻辑思维能力；与此同时每两周给老板发一次周报，注意周报的表述方式，尽量让老板想看，且看一遍就能懂！ 培养兴趣，陶冶情操 分析自己抓不住重点的原因，应该和情商低有关，表达时不知道别人最想知道什么，因此也不很好的向别人转述自己的想法。为了提高情商，陶冶情操，年后我开始了在科研之余练习书法，学习唱歌，看小说，下一步准备学吉他和非洲鼓！ 改掉恶习，非一朝一夕之功，有很多地方仍然做的不太好，但是已有一定成效，滴水穿石，贵在坚持！ 最后，这是老板之前对我说的一番话：“人生就是这样，没有如果，只有结果！人生是残酷的，没人会同情弱者，要努力让自己变强，增加自己的魅力，遇事莫等人，自强不息！” 共勉之！","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"安和桥","date":"2017-04-20T06:17:37.131Z","path":"2017/04/20/安和桥/","text":"不得不说，一段非洲鼓奏乐令人印象深刻 敲鼓节奏 歌词 12345678910111213141516171819202122232425262728293031323334353637让我再看你一遍从南到北像是被五环路蒙住的双眼请你再讲一遍关于那天抱着盒子的姑娘擦汗的男人我知道 那些夏天就像青春一样回不来代替梦想的也只能是勉为其难我知道 吹过的牛逼也会随青春一笑了之让我困在城市里纪念你让我再尝一口秋天的酒一直往南方开不会太久让我再听一遍最美的那一句你回家了我在等你呢（music）我知道那些夏天就像青春一样回不来代替梦想的也只能是勉为其难我知道吹过的牛逼也会随青春一笑了之让我困在城市里 纪念你我知道那些夏天就像你一样回不来我已不会再对谁满怀期待我知道这个世界每天都有太多遗憾所以 你好 再见","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"VSCode配置","date":"2017-04-19T03:34:34.450Z","path":"2017/04/19/VSCode配置/","text":"工欲善其事必先利其器，学长都推荐source insight来看代码，用了一段时间，感觉还是不太方便，于是开始转战VS Code了！VS Code确实是一个不错的编辑器，而且支持Linux/Mac Os/Windows系统。 为什么用它看代码会比较方便呢？比如我要看下面这个变量buswait的定义(图1)，只需要按住ctl点击该变量即可转到定义(图2)，是不是跟eclipse的功能有点像呢！！！对于我这种JAVA转战C的码农简直就是天降福利啊，而且还支持分屏显示(图3)！！！ 图1.图2. 图3. 当然VS code还可以用来编译调试C/C++代码，下面说一下具体配置过流程 流程： 下载安装vscode 安装cpptools插件 安装编译、调试环境 修改vscode调试配置文件 下载安装vscodehttps://code.visualstudio.com/Download 安装VSCode的C++插件Ctrl+P，再输入ext install cpptools 进行安装。 安装TDM-GCC(编译调试环境)VSCode默认不带编译和调试环境的，需要自行安装编译器等。 我选用的是TDM-GCC编译套件，方便易用。（Codeblocks、Dev-Cpp等默认带的都是这款）。 下载地址：http://tdm-gcc.tdragon.net/download 根据自己的情况下载安装即可，会自动添加路径到环境变量PATH。 ==重启电脑，让环境变量生效！！！== 配置VSCode 新建test.cpp 1234567891011#include &lt;iostream&gt;using namespace std;int main(int argc, char** argv)&#123; cout &lt;&lt; &quot;Hello World! &quot; &lt;&lt; endl; system(&quot;pause&quot;); return 0;&#125; 按F5，打开launch.json文件，编辑并保存 123456789101112131415161718192021&#123; &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;C++ Launch&quot;, // 配置名称，将会在启动配置的下拉菜单中显示 &quot;type&quot;: &quot;cppdbg&quot;, // 配置类型，这里只能为cppdbg &quot;request&quot;: &quot;launch&quot;, // 请求配置类型，可以为launch（启动）或attach（附加） &quot;launchOptionType&quot;: &quot;Local&quot;, // 调试器启动类型，这里只能为Local &quot;targetArchitecture&quot;: &quot;x86&quot;, // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 &quot;program&quot;: &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;, // 将要进行调试的程序的路径 &quot;miDebuggerPath&quot;: &quot;gdb.exe&quot;, // miDebugger的路径，注意这里要与MinGw的路径对应。64位系统用gdb64.exe &quot;args&quot;: [ &quot;&quot; ], // 程序调试时传递给程序的命令行参数，一般设为空即可 &quot;stopAtEntry&quot;: false, // 设为true时程序将暂停在程序入口处，我一般设置为true &quot;cwd&quot;: &quot;$&#123;workspaceRoot&#125;&quot;, // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 &quot;externalConsole&quot;: true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 &quot;preLaunchTask&quot;: &quot;g++&quot; // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; 按F1键，出来输入框后，输入task，选择“配置任务运行程序”，再随便选一项，编辑并保存tasks.json： 12345678910111213141516171819202122232425&#123; &quot;version&quot;: &quot;0.1.0&quot;, &quot;command&quot;: &quot;g++&quot;, &quot;args&quot;: [ &quot;-g&quot;, &quot;$&#123;file&#125;&quot;, &quot;-o&quot;, &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot; ], // 编译命令参数 &quot;problemMatcher&quot;: &#123; &quot;owner&quot;: &quot;cpp&quot;, &quot;fileLocation&quot;: [ &quot;relative&quot;, &quot;$&#123;workspaceRoot&#125;&quot; ], &quot;pattern&quot;: &#123; &quot;regexp&quot;: &quot;^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$&quot;, &quot;file&quot;: 1, &quot;line&quot;: 2, &quot;column&quot;: 3, &quot;severity&quot;: 4, &quot;message&quot;: 5 &#125; &#125;&#125; 在.vscode目录下新建文件c_cpp_properties.json，编辑并保存： 12345678910111213141516171819202122&#123; &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;Win32&quot;, &quot;includePath&quot;: [ &quot;D:/Program Files/TDM-GCC-64/include&quot;, &quot;D:/Program Files/TDM-GCC-64/x86_64-w64-mingw32/include&quot;, &quot;D:/Program Files/TDM-GCC-64/lib/gcc/x86_64-w64-mingw32/5.1.0/include&quot;, &quot;D:/Program Files/TDM-GCC-64/lib/gcc/x86_64-w64-mingw32/5.1.0/include/c++&quot; ], &quot;browse&quot;: &#123; &quot;limitSymbolsToIncludedHeaders&quot;: true, &quot;databaseFilename&quot;: &quot;&quot; &#125; &#125; ], &quot;clang_format&quot;: &#123; &quot;style&quot;: &quot;file&quot;, &quot;fallback-style&quot;: &quot;LLVM&quot;, &quot;sort-includes&quot;: false &#125;&#125; 按F5进行调试，结果如下 参见Dreamlike博客","tags":[{"name":"VSCode","slug":"VSCode","permalink":"http://yoursite.com/tags/VSCode/"},{"name":"c++","slug":"c","permalink":"http://yoursite.com/tags/c/"}]},{"title":"存储相关-3-磁盘阵列以及存储系统(DAS、SAN和NAS)","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/存储相关-3-磁盘阵列以及存储系统(DAS、SAN和NAS)/","text":"磁盘阵列多个磁盘通过RAID卡组合起来,组成JBOD（just a bound of Disks,一串磁盘）.JBOD称为磁盘柜,凡是自带RAID控制器的盘柜就叫做磁盘阵列或者盘阵. 双控制器常用两个控制器来保证磁盘阵列的安全性 Active-Standby热备份 一个作为备份，只有当主控制器故障时副控制器才工作 Dual-Active双控制器同时工作两个同时工作，当其中一个故障时，另一个仍能正常接管整个阵列的管理. Split Brain脑分裂其中一个控制器检测不到另一个控制器的存在,则向电源控制器发送信号，重启对方并进入Standby状态. DAS、SAN和NASDAS (Direct Attached Storage)直接附加网络直连式存储与服务器主机之间的连接通道通常采用SCSI连接，随着服务器CPU的处理能力越来越强，存储硬盘空间越来越大，阵列的硬盘数量越来越多，SCSI通道将会成为IO瓶颈；服务器主机SCSI ID资源有限，能够建立的SCSI通道连接有限。由于存储设备通过电缆直接与计算机相连，系统存取访问I/O请求直接在计算机和存储设备之间进行，故会受如下因素制约 硬盘 内存缓存 虚拟内存 存储控制器 RAID级别以及读写模式 总线长度 DAS面临的挑战 存储与主机必须直连 数据可用性 单节点故障 数据资源的共享 可扩展性有限 连接的端口和主机有限 有限的可寻址范围（即容量有上限） 距离限制 需要停机维护 NAS(Network Attached Storage)附网存储由于DAS存在上述的一系列限制,网络存储渐渐浮出水面,并得到了广泛应用. 网络存储的本质是通过网络建立用户与存储设备之间的连接，通过网络传输数据. 技术上通过软件提供高性能的文件服务. NAS本质是一个专用的文件服务器，NAS不一定是盘阵，一台主机只要它有自己的磁盘和文件系统，并向外提供访问文件系统的接口（如NFS、CIFS等），就可以做成NAS. NAS其实就是处于以太网上的一台利用NFS、CIFS等网络文件系统的文件共享服务器. 一种瘦服务器方式的存储设备 NAS传输协议 CIFS(Common Internet File System) 微软定义的一套规范 基于Windows NT的公共互联网文件系统 使用TCP协议 在服务器端验证用户身份，比NFS安全 NFS(Network File System) Unix/Linux使用的协议 底层使用TCP或者UDP协议 开销远远小于CIFS 在用户端安全登录 SAN（Storage Area Net）存储局域网 SAN采用网络互连的存储区域网. FC-SAN得到了广泛应用，使用FC协议网络通讯. SAN只支持SCSI协议，SCSI语言及数据可以用FC协议传递。 NAS和SAN对比 速度对比除非NAS使用快于内存的网络方式与主机通讯，否则其速度永远无法超过SAN架构。但是如果后端磁盘有瓶颈，那么NAS用网络代替内存的方法产生的性能降低就可以忽略。 成本对比NAS比SAN成本低很多 可扩展性NAS可扩展性强于SAN，只要有IP的地方，NAS 就可以提供服务，且容易部署和配置。NAS设备一般都可以提供多种协议访问数据，而SAN只能使用SCSI协议访问。 数据共享NAS可以被多个客户端访问，SAN除非安装了专门的集群管理系统或集群文件系统模块，否则不能共享某个LUN. SAN和NAS的选择CPU不密集但是大块连续IO密集的应用选择SAN比较合适对于IO不密集的随机小块IO场景选择NAS，NAS的根本瓶颈是底层链路的速度，若为高速以太网,首选NAS. 备注目前普遍的架构是文件系统和磁盘控制器驱动程序都运行在应用服务器主机上。文件系统向卷发送的请求是通过内存来传递的，而主机向磁盘（LUN）发送的请求是通过FC网络来传递的。（SAN）","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"存储阵列","slug":"存储阵列","permalink":"http://yoursite.com/tags/存储阵列/"},{"name":"存储系统","slug":"存储系统","permalink":"http://yoursite.com/tags/存储系统/"}]},{"title":"排队论在ad hoc无线网中的应用","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/排队论在ad hoc无线网中的应用/","text":"Applications in Analysis of Ad Hoc Networks using Queuing Theory Yu Zhang Abstract Queuing problems are common scenarios in our daily life. From industrial manufacture to daily queuing, we can never leave this topic. Ad Hoc wireless networks are new kinds of networks. It has the advantages of high flexibility and extensibility, which are applied to our daily life gradually. The paper lists the applications in performance analysis of Ad Hoc networks via using queuing theory, and discusses their pros and cons as well as their development directions. At the end of the article, we reproduce some of the experiments via using MATLAB simulators and give a detailed discussion on the results. IntroductionQueuing Theory is a model discusses a dynamic service system’s running processes. In this system, the arrival of the customers is an exact stochastic process. When they arrive at the system, they will queue until it’s time for their service. After accepting service, customers are always supposed to leave. For instance, the M/M/1/∞ model (shown in Fig.1) is a classical model used commonly. In this model, we assume the arrival of the customers is an i.i.d. Poisson process and the time interval of only server is an exponential distribution. More importantly, there is an infinite queue length. Many complex models are its variations. Fig.1 A classical M/M/1 queuing model. In this model, we assume the arrival of the customers is an i.i.d. Poisson process and the time interval of only server is an exponential distribution. Ad hoc are networks without any base stations, i.e. infrastructure-less. Similar to the ZigBee protocol, they are self-organizing and adaptive networks, who allow spontaneous formation and deformation of networks topology. If node A can connect directly with node B, then they will save the other’s information at their own routing table, respectively. Another scenario is that A can only connect with B via intermedia C (shown in Fig. 2), then all of their routing tables will be updated to ensure that A can sense the existing of B. There are two topologies of ad hoc networks. One is heterogeneous, which means all the nodes have different capabilities. The other is homogeneous, i.e. all nodes have identical capabilities and responsibilities. Fig.2 Ad hoc network rules. If A can only connect with B via intermedia C, then all of their routing tables will be updated to ensure that A can sense the existing of B. There are great number of researches on ad hoc networks, and we can divide them into two kinds. On is to improve the performance via optimizing the protocol, e.g. MACA [1] and MACAW [2]. The former one uses the RTS/CTS mechanism to avoid the hidden terminal problem. And the latter one is an optimization of the first one who can solve the problem of exposed terminal. The other researches are on the problem of which protocol is the best, i.e. how to evaluate the specific system. And we will discuss this topic latter. The rest of the article is organized as follows. The second part of the article (section two) introduces applications in performance analysis of Ad Hoc networks via using queuing theory. The third part of the article (section three) we reproduce some of the experiments via using MATLAB simulators and give a detailed discussion on the results. Finally (section four) is the conclusions. Performance Analysis of Mobile Ad Hoc Networks using Queuing TheoryPerformance studies of ad hoc networks usually focus on either analytical modelling or simulations. We mainly discuss the evaluations via using Queuing theory. There are various publications on this topic. For instance, Jaehyuk Choi [3] analyzes the blocking probability and the queuing delay in the closed loop queue networks with finite queue length by using simulators. Daniele Miorandi [4] analyzes various performance indicators by of the wireless networks with HTTP traffic load by utilizing queuing models. Mustafa Ozdemir [5] analyzes how to design an efficient ad hoc network by using M/MMGI/1/K model. Saikat Ray [6] evaluates the mean delay and collision rate in ad hoc networks with hidden stations by using queuing simulators.We mainly discuss the Nabhendra Bisnik’s research on evaluating the ad hoc networks by establishing a G/G/1 network model, which seems to be more practical. The basic topology of the model [7] is shown in Fig. 3. The main idea of this paper is using diffusion approximation [8] to solve the G/G/1 model. The variables mentioned in this paper are shown as follow. The visit ratio of node i, denoted by ei, is given by e_i= p_0i (n)+ ∑p_ji (n)×e_j (1) The resulting arrival rate is termed the effective arrival rate at a station. The effective arrival rate at a node i, denoted by ‚i is given by λi= λ(e )× e_i (2) The utilization factor of station i, denoted by ρi, is given by ρ_i= λ_i ⁄ μ_i (3) Then by using the diffusion approximation, the paper get the closed form expressions of the mean delay (Formula 4) and the maximum throughput (Formula 5) of the model. D(n) = ρ_i⁄((λ×(1-ρ)) ) (4) λ_max= p(n)⁄((1/ξ+L/W+4nA(n) L/W ) ) (5) Fig. 3. The G/G/1 model of ad hoc networks with stationary nodes. Figure (a) shows the global nodes where P¬ij(n) means the probability that a package will transfer from node i to node j. While Figure (b) shows a single node in (a), and p(n) means the probability that the destination of a package is this node. From the above formulas, we can get the knowledge of the interplay of delay (or maximum throughput) and the visit ratio, the utilization factor and so on. However there are also some drawbacks in this research. For example, the mobile nodes are always mobilizable rather than stationary. What’s more, the queue length is always infinite in the real world rather than finite. So there are other researches to optimize this work, e.g. Aznida Hayati Zakaria [9] analyze a practical protocol with finite queue length via using their queue models, more information can refer to the article. SimulationFor the sake of simplicity, we consider an ad hoc network with only two mobile nods (Fig.4), and more complex scenarios are similar. Fig. 4 The simplest ad hoc network model with only two mobile nodes. As we can see in the picture, a1 means the arrival rate of Node1 while d1 is the departure rate, and p1 is the possibility that the page will be delivered to Node2. Node2 is the same. We can get there are nine states in this model. We express these states in matrix S, whose element S(i,j) means there are i packages in Node1 and j packages in Node2. s= (0,0) (0,1) (0,2) (1,0) (1,1) (1,2) (2,0) (2,1) (2,2) Easily, we can get the state-transition diagram (Fig. 5). Fig.5 The state-transition diagram. It the basis of our theoretical analysis. From the above diagram, we can list the balanced equation, and get the stability probabilities of each state.By using MATLAB, we analysis the package missing rate as well as throughput of each node. In the simulation, we assume that a1=1, a2=10, d1=6, d2=4, p1=2, p2=3, all of which can be got beforehead in real word. Fig.6 and Fig.7 are the simulation results, which denote the relationship between arrival rate (AR) and package missing rate (PMR). Fig.6 The relationship between arrival rate (AR) of node1 and its own package missing rate (PMR). Fig.7 The relationship between arrival rate of node2 and package missing rate of node1. From the simulation result, we can get that a node’s PMR increases as its own AR increases (Fig.6), while the influence from other nodes’ AR is negligible (Fig. 7). We have also found that a node’s throughput increases until an upper limit as its own AR increases (Fig 8). Fig.8 The relationship between AR and throughput. ConclusionsWe first introduce the queuing theory briefly. Then we list the applications in performance analysis of Ad Hoc networks via using queuing theory, and discusses their pros and cons as well as their development directions. Finally we reproduce some of the experiments via using MATLAB simulators and give a detailed discussion on the results, i.e. a node’s PMR increases as its own AR increases, while the influence from other nodes’ AR is negligible. We have also found that a node’s throughput increases until an upper limit as its own AR increases. Reference[1] P. Karn. MACA: a new channel access methos for packet radio. In Proceedings of the 9th Comouter Networking Conference, pages 134–140, September 1990. [2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW: A media access protocol for wireless LANs. In SIGCOMM, pages 212–225. ACM Press, 1994. [3] G. Zeng, H. Zhu, and I. Chlamtac. A novel queueing model for 802.11 wireless LANs. In Proceedings of WNCG Wireless Networking Symposium, 2003. [4] D. Miorandi, A. A. Kherani, and E. Altman. A queueing model for HTTP traffic over IEEE 802.11 WLANs. In Proceedings of 16th ITC Specialist Seminar, August 2004. [5] M. Ozdemir and A. B. McDonald. An M/MMGI/1/K queuing model for IEEE 802.11 ad hoc networks. In Proceedings of PE-WASUN’05, pages 107–111. ACM Press, 2004 . [6] S. Ray, D. Starobinski, and J. B. Carruthers. Performance of wireless networks with hidden nodes: A queuing-theoretic analysis. To appear in Journal of Computer Communications. [7] N. Bisnik and A Abouzeid. Queuing Network Models for Delay Analysis of Multihop Wireless Ad Hoc Networks. International Wireless Communications &amp; Mobile Computing Conference, 2006. [8]G. Bolch, S. Greiner, H. de Meer, and K. S. Trivedi. Queuing Networks and Markov Chains, chapter 10, pages 423–430. John Wiley and Sons, 1998. [9]A. H. Zakaria, Y. M. Saman, A. S. Noor, etc. Performance Analysis of Mobile Ad Hoc Networks using Queuing Theory. Proceedings of the First International Conference on Advanced Data and Information Engineering, 2013. Original works, banned reproduced! If use, please indicate the source! ! ! 原创作品，禁止转载！若是转载，请注明出处！","tags":[{"name":"排队论","slug":"排队论","permalink":"http://yoursite.com/tags/排队论/"},{"name":"ad hoc无线网","slug":"ad-hoc无线网","permalink":"http://yoursite.com/tags/ad-hoc无线网/"}]},{"title":"存储相关-4-虚拟化与系统IO路径优化","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/存储相关-4-虚拟化与系统IO路径优化/","text":"几个概念 HPC（High Performance Computing）高性能计算通过增加整个计算系统CPU总核心数，可以成倍缩短执行时间 HAC（High available Computing）高可用性集群通过备份，当一活动节点发生故障时，备份可用，不会影响整个系统的使用 LBC负载均衡计算 理解主机端IO路径架构应用程序层 1. NFS下的缓存机制 默认mount参数下的IO 默认条件下使用异步(async)方式，rsize=wsize=65535。内核不会透传程序的IO给NFS Server,对于写IO会延缓执行，积累一定的时间以便合并上层的IO。不管读还是写，async方式都会具有一定的效果，尤其是连续的IO地址。 Linux下使用NFS，对于写操作，不管offset是否为Page或者512B对其，都没有任何写惩罚存在，对于读操作，也只在随机读的情况下出现了读惩罚，其他任何情况下都没有惩罚. 可以使用dd命令测试，dd是一个使用同步调用+buffered IO模式的程序.dd if=/mnt/3 of=/dev/null bs=1500 count=100dd为同步调用，到了底层，内核将dd的写IO数据合并，并且以异步的方式高效的发送给NFS服务器. 指定同步（sync）参数需要了解，内核的异步过程，只对Buffered IO模式下的同步写、异步写、异步读有意义.对于读立即执行，不一定表示不可以进行Prefetch和Cache Hit操作，但是对于写立即执行，却绝对不可以将待写的数据缓存起来延迟处理。 指定rsize/wsize参数rsize/wsize，对于NFS的IO逻辑没有任何影响，收到影响的只是底层传输的数据包数量和大小任何情况下均不要降低rsize或者wsize,百害而无一利。 使用O_DIRECT参数由于Linux下的NFS有很明显的预读力度。只在特定的条件下（小的随机读）有读惩罚，写惩罚一点也没有，因此很少使用DIO选项，除非应用中已经做了非常完善的缓存策略（如许多数据库就是如此）. 多进程访问下的缓存一致性解决方法Linux下的NFS缓存一致性，从严格到不严格一次为使用DIO模式、使用noac选项来mount、降低actimeo阈值到尽量低的值、默认mount参数。GETATTE是多客户端访问环境下NFS实现缓存一致性的法宝。###文件系统层文件系统一个最大的任务就是负责在逻辑文件与底层卷或者磁盘之间做映射，并且维护和优化这些映射信息。文件系统还需要负责向上层提供文件IO访问API接口，比如打开、读、写、属性修改、裁剪、扩充、锁等文件操作。另外，还需要维护缓存，包括预读、Write Back/Write Through/Flush等操作；还需要维护数据一致性，比如Log、FSCK等机制；还需要维护文件权限、Quota等。可以把一个文件系统逻辑上分为以下三个部分。 上部（访问接口） 中部（缓存管理、文件管理） 下部（文件映射、一致性保护、底层存储卷适配） 写命中是指写入的数据对应的地址在缓存中恰好在之前的尚未被写盘的IO数据。 卷管理层 块设备卷管理在某种程度上是为了弥补底层存储系统的一些不足之处，比如LUN空间的动态管理等。卷管理最大的任务是做Block级的映射，对于IO处理，卷层只是做一个将映射翻译之后的IO向下转发的动作以及反向过程。即使上层的IO不是4KB对齐，底层的IO也应该是4KB对齐的。 读惩罚当某个读IO请求的IO Size不可被OS Page Size或者Disk Sector Size整除时，这个读请求就会产生读惩罚。但是如果边界未4K对齐，仍然可能产生读惩罚。写惩罚写惩罚的表现是既有额外的读操作，又有额外的写操作。比读惩罚浪费更多的资源。 字符设备 概述传统的字符设备是转指一类接受字符流的物理终端、键盘等。这种设备可以直接对设备进行底层的操作而不使用缓存，而且每次IO必须以一个字符为单位。卷抽象出来的字符设备只是抽象出来了字符设备所具有的特点。对字符设备进行IO操作必须遵循底层的最小单位对齐规则，比如对于卷字符设备来说，每个IO长度只能是扇区。底层发出的IO长度与应用层发出的IO长度一一对应。 裸设备字符设备又被称为裸设备。寓意在于可以直接对裸设备进行IO操作，只不过需要自行维护数据，如扇区映射以及预读缓存等。由于使用文件系统缓存以及内核缓存容易造成数据的不一致性且容易造成读写惩罚，对于IO性能要求高的程序，可以直接操作底层的物理设备。 Direct IO如果即不想使用FS的缓存以防造成读写惩罚，又想利用文件系统的其他功能，则可以使用DIO模式。这里要注意一个概念，操作系统内有很多缓存，而用户程序的第一处缓存并不是通常理解的FS缓存，而是SystemBuffer。因为每发起一个IO请求，操作系统会根据程序IO请求的空间在操作系统内存空间分配相同大小的内存用于充当SystemBuffer，然后再往下才是文件系统缓存。 设备驱动层 SCSI设备驱动链其流程图如下 可以使用iostat工具监控IO状况。还可以使用blktrace对块级IO进行监控。 ATA设备驱动链Linux使用一种LibATA库来负责将SCSI协议转换为ATA协议并发送给ATA控制器驱动程序。 注意块设备由于底层IO Scheduler可能造成IO乱序重排执行情况，在发生系统宕机情况时，底层数据的一致性就无法得到保证。由于文件系统建立在块设备上，所以FSCK是一种恢复数据一致性的手段，但FSCK只能保证文件系统元数据的一致性却保证不了数据实际内容的一致性，所以，关键应用程序都直接使用完全透传程序IO请求的字符设备进行IO操作。 IO路径中Queue Depth和Queue LengthQueue Depth指的是Queue的最大长度，而Queue Length指的是Queue的当前长度。Queue Length在一定程度上反应了当前系统的忙闲程度，监测到的Queue Length越长，就证明Queue积压越大，那么单个IO的延迟也就越高。 IO Latency = （Queue Length + 1） IO Service Time = （Queue Length + 1） 其下层的IO Latency 如果IOPS尚未达到额定饱和值而Queue积压，那么说明瓶颈归于磁盘；如果IOPS饱和，说明瓶颈归于存储控制器的处理能力；如果带宽饱和，那么瓶颈归于链路本身。 time [-p] command [arguments…]time命令可以统计命令执行的时间 Queue Length 和 IOPS的关系IOPS = Queue Length / Average Response Time(ART) IO Size和IO延迟的关系IO Size越大，从盘片读写以及传输每个IO所用的时间就越长。所以，Write Merge操作其实增加了每个IO的延迟，但是它提高了效率，提高了系统的吞吐量。一些网络技术，如Infineband，其传输单元小，对于一些小的IO操作能够实现较高的响应速度，适合高实时性的传输，但是其整体的吞吐率不一定比其他传输技术高。 面向SSD的Queue优化需要更大的Queue Depth。","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"虚拟化","slug":"虚拟化","permalink":"http://yoursite.com/tags/虚拟化/"},{"name":"IO路径优化","slug":"IO路径优化","permalink":"http://yoursite.com/tags/IO路径优化/"}]},{"title":"存储相关-2-RAID、虚拟磁盘、卷和文件系统","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/存储相关-2-RAID、虚拟磁盘、卷和文件系统/","text":"七种RAIDRAID以4个扇区组成的块作为基本单位，不同磁盘的相同偏移处的块组成Stripe，也就是条带.对于RAID 0 磁盘组，如果有大块数据写入时，则数据在很大几率上可以以条带为单位写入，并行写入能大大提高写入速度. 简介 RAID 0没有冗余，通过并行写入提高性能. RAID 1 RAID 0 数据不安全，一块盘坏掉，整个阵列就坏了.因此RAID 1 拿另一磁盘做备份，即一次性写两份. 读性能和顺序写性能提高N倍，但随机写由于公用同一块备份盘，因此性能没有提升. RAID 2 采用Hamming Code ECC进行校验.汉明码只能纠正一位错误.所以只能允许一块硬盘出问题.如果两块或以上硬盘出现问题，RAID 2的数据将被破坏. 其实是将原本连续的一个扇区数据，以位为单位分割存放在不连续的多块物理盘上.也就是条带深度为1. RAID 2不能实现并行IO，因为每次IO都要联动所有的磁盘，只能做到一个IO内部的并行. 基于其特点，能将顺序读写提升N倍，随机读写不能提升. RAID 3 采用异或的性质来求校验值.对于RAID 3 连续读写性能几乎是单盘的N倍.随机读写并没有提升，几乎和单盘一样. RAID 3 执行一次IO必须牵动占用所有盘，其他IO就必须等待，因此根本不能并发IO. 一般来说，RAID 3的条带长度=文件系统的块大小，避免条带不对齐的现象. 与RAID 2相同，只对大的连续读写有提升，小的随机读写没有优化. RAID 4 由于实际的读写常常是小的随机读写，前四种做法会有明显的缺陷，因此出现了RAID 4. RAID 4通过增大条带深度，大大增大了IO读写的并发性. RAID 5 由于RAID 4写请求需要共享校验盘，因此写请求不能实现并发.因此RAID 5将校验数据分散在各个磁盘上.使并发概率最大化. 新校验码=老数据XOR新数据XOR老校验码. 如果之前校验码出错，只能用新数据重新计算校验码，否则新校验码仍为错误. RAID 5磁盘数越多，可并发的几率越大. RAID 6 通过增加一个校验盘提高安全性，可以同时坏两块盘. 注意 如果随机小块IO多，则适当加大条带深度；如果连续大块IO多，则适当减少条带深度. 操作系统中RAID实现为了保证性能,同一磁盘组只能用同类型的磁盘,混合使用多类型磁盘组成虚拟磁盘非特殊要求不会有这样的设计.这种RAID称为软件RAID,存在如下缺点：①占用内存空间；②占用CPU资源（如利用异或求检验值等）；③软件RAID程序无法将安装有操作系统的那个磁盘分区做成RAID模式.RAID程序运行在操作系统之上,在启动操作系统之前无法实现RAID功能,因此操作系统损坏,RAID程序也无法运行,数据丢失. RAID卡其实是一种硬件实现RAID. 0通道RAID卡 自身没有SCSI通道,利用主板上已经集成或者已经插在PCI上的SCSI卡,来控制他们的通道. 无驱动RAID卡使用SATA接口连接计算机. RAID On Chip(ROC)技术利用SCSI卡上的CPU处理芯片,通过在SCSI卡的ROM中加入RAID代码而实现. RAID卡上内存用于存放CPU执行代码以及作为数据缓存.#虚拟磁盘 操作系统如何看待逻辑盘 目前各种RAID卡都可以划分逻辑盘，逻辑盘大小任意设置.每个逻辑盘对于OS来说都是一块单独的物理盘.而分区OS在一块物理磁盘上做的再次划分. 卷管理由于虚拟磁盘存在不灵活性,扩展上存在缺陷,因此出现了卷管理(Volume Manager,VM).很多操作系统上都有逻辑卷管理（LVM）.其主要功能是将OS识别到的物理磁盘（RAID卡虚拟化的逻辑盘）进行在组合. 相关概念 PV ： OS识别到的物理磁盘，叫物理卷（physical volume） VG ： 多个PV被逻辑的放到一个卷组(volume group)中 PP：即物理区块（physical partition）.是在逻辑上再将一个VG分割成连续的小块.逻辑上连续的PP物理上不一定连续. LP ： 逻辑区块.为实现某种功能,由多个PP组成的一个集合. LV ：由多个LP组成的逻辑卷. 总结起来就是将多个PV组合成一个存储集合VG,并在逻辑上将VG划分成若干PP.然后再在逻辑上组成若干LV.其中LV的基本单元是LP.而LP的基本单元是PP. 高级卷管理和低级卷管理 分区管理可以看做一种最简单的卷管理方式，它比LVM等要低级，比如windows自带的磁盘管理器.这些卷信息都存放在LBA1扇区上,这个扇区叫做主引导记录MBR,MBR还保存了BIOS跳转时所需要的第一句指令代码. 高级卷管理在划分逻辑卷之后,一定要记录逻辑卷是怎么划分的.LVM使用VGDA（volume group descriptior area）存放卷信息. 文件系统在一个没有文件系统的计算机上,如果一个程序要向磁盘上存储一些自己的数据,那么这个程序只能自己调用磁盘控制器驱动,或者调用VM提供的接口,对磁盘写数据.引入文件系统后,各个程序之间都通过文件系统接口访问磁盘,所有被写入的数据都称为一个文件,有名字,是一个实体.文件系统的IO包括同步IO、异步IO、阻塞IO/非阻塞IO和Direct IO.","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"RAID","slug":"RAID","permalink":"http://yoursite.com/tags/RAID/"},{"name":"文件系统","slug":"文件系统","permalink":"http://yoursite.com/tags/文件系统/"}]},{"title":"在github pages上建站","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/在github pages上建站/","text":"好记性不如烂笔头，计算机科学日新月异，养成写博客的习惯对于程序猿来说尤为关键，于是便搭了自己的博客！ 建站过程中参见的博客有使用Hexo建站十分便利，下面是建站过程中主要参见的博客 手把手教你使用Hexo + Github Pages搭建个人独立博客 在Windows平台上安装Node.js及NPM模块管理 GithubPages+Hexo博客主题Yilia yilia作者的使用建议 非常有用的Markdown 编辑器和工具 Markdown 语法说明 (简体中文版) 献给写作者的 Markdown 新手指南 添加访客数量统计 添加站内搜索 添加返回顶部功能 其中值得注意的是，下载完git工具后，需要输入下面两条命令来设置github账号和邮箱！！！！然后会有弹框要求输入用户名和密码，输入正确即可正常部署了！！！否则会出下图的错误！ 12git config --global user.email &quot;you@example.com&quot;git config --global user.name &quot;Your Name&quot;","tags":[{"name":"github pages","slug":"github-pages","permalink":"http://yoursite.com/tags/github-pages/"},{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"存储相关-1-总线与存储器","date":"2017-04-03T11:19:36.349Z","path":"2017/04/03/存储相关-1-总线与存储器/","text":"1 IO总线分类 仲裁总线（控制总线） 地址总线 数据总线 PCI总线 PCI总线是目前普遍使用的一种连接南桥与外设的总线技术. PCI总线式数据与地址总线分时复用的. PCI总线所有时序的产生与控制都由Master发起，同一时刻只能供一对设备完成传输. PCI总线共享：硬件上使用三极管的物理特性实现，软件上通过中断链的方法实现. IO总线的结构 CPU与内存之间存在一个北桥芯片，这个芯片连接了CPU、内存与IO总线.CPU与北桥连接的叫系统总线（或者前端总线）.内存与北桥连接的叫内存总线.北桥与IO总线之间会有一个南桥芯片，协调北桥与IO总线之间的速度差.南桥负责连接控制众多的外设. 磁盘控制的指令集目前有ATA和SCSI指令集（或叫协议），SCSI比ATA指令集高效，所以广泛用于服务器和磁盘阵列环境. 2 磁盘原理与技术硬盘大致由盘片、读写头、马达、底座、电路板等几大项组成.盘片基板要求表面光滑平整，不可有任何瑕疵.磁粉需要不含杂质，并均匀地溅渡在磁盘上.磁头利用空气动力学使磁头悬浮于盘面之上.为了让磁头精确定位到每个磁道必须用步进电机，利用精确的齿轮组或者音圈，每次旋转可以仅仅使磁头进行微米级的位移. 盘上的数据组织 逻辑上分为磁道、柱面、扇区. 磁道从外圈向内圈从0开始编号，每一盘面有300~1024个磁道.磁道上每段圆弧叫做一个扇区.扇区从1开始编号，每个扇区中的数据作为一个单位同时读出或者写入，是读写的最小单位. 每个盘面会有一个磁头.只有同一柱面内从0磁头开始进行操作，向下所有柱面数据都读完时，磁头才会到下一个柱面，因此写数据都是先写满一个柱面，然后再写下一个柱面. 柱面（Cylinder）、磁头（Header）和扇区（Sector）三者简称CHS，所以扇区地址又称为CHS地址. 扇区的排列不是顺序的，而是根据交叉因子去调节的. SCSI接口完成了访问磁盘过程的虚拟化和抽象，极大的简化了访问磁盘的过程，它屏蔽了磁盘内部结构和逻辑. 磁盘相关高层技术 排队技术 无序传输技术 几种可控磁头扫描方式 FCFS(First Come First Serve) SSTF(Shortest Seek Time First) 控制器优先让磁头调到离当前磁头位置最近的IO磁道读写.存在不公平性，导致IO饿死. SCAN（回旋扫描模式） C-SCAN（单向扫描模式） LOOK(智能监控扫描模式) 与SCAN模式的区别在于，磁头不必到达终点之后才折返，而是完成最两端的IO即可折返. 在高负载的条件下，SCAN或者C-SCAN、C-CLOCK模式更为适合. 磁盘缓存 用来接收指令和数据，还用来进行预取.磁盘缓存表现为一块电路板上的RAM芯片，目前有2MB、8MB、16MB、32MB、64MB等容量规格. SCSI指令集中有两个参数可以控制对磁盘缓存的使用.DPO（Disable Page Out）和FUA（Force Unit Access）,当这两个参数都被设置为1时，相当于完全不使用磁盘缓存，但是指令和数据依然会先到达缓存中. 影响磁盘性能的因素 磁盘每个时刻只允许一个磁头来读写数据，因此多个磁头不可能提高磁盘的吞吐量和IO性能. 影响硬盘性能的因素包括：转速、寻道时间、单碟容量（越高，相同开销下读出数据越多）、接口速度（影响最小）. 硬盘接口技术 分类 用于ATA指令系统的IDE接口 用于ATA指令系统的SATA接口 用于SCSI指令系统的并行SCSI接口 用于SCSI指令系统的串行SCSI(SAS)接口. 用于SCSI指令系统的IBM专用串行SCSI接口(SSA). 用于SCSI指令系统的并且承载于FabreChannel协议的串行FC接口（FCP）. IDE硬盘接口技术 Integrate Drive Electronics 电子集成驱动器.即把控制电路和盘片、磁头等放在一个容器中的硬盘驱动器. 也叫PATA接口. ATA英文拼写为Advanced Technology Attachment，即高级技术附加. ATA-3引入了S.M.A.R.T(self-monitoring analysis and reporting technology，自监测、分析和报告技术). ATA-4和ATA-5分别叫ATA33和ATA66，意思为传输速度33.3MB/s和66.6MB/s，同时引入了CRC校验. ATA-7是ATA的最后一个版本,支持133MB/s的IDE硬盘，迈拓是唯一一家推行该协议的厂家.大部分都使用Serial ATA接口标准. IDE数据传输模式 PIO模式（Programming Input/Output Model）: 一种通过CPU执行I/O端口指令来进行数据读写的数据交换模式，是最早的硬盘数据传输模式.这种模式有较高的CPU占有率. DMA模式（Direct Memory Access）: 是一种不经过CPU而直接从内存存取数据的数据交换模式. Ultra DMA模式 ： 高级直接内存访问 SATA硬盘接口 SATA 2.0规范中添加的新特性包括：3Gb/s的传输速率；支持NCQ技术（Native Command Queuing ，自身命令队列）；端口选择器（Port Selector）；端口复用器（Port Multiplier）可以在一个控制器上扩展多个SATA设备 ；服务器特性；接口和连线的强化. SCSI硬盘接口对于ATA协议，对应的就是IDE接口；对于SCSI协议，对应的就是SCSI接口.SCSI全程Small Computer System Interface，即小型计算机系统接口.是一种具备与多种类型外设进行通行的能力，如硬盘，CD-ROM，磁带机和扫描机等. SCSI采用ASPI（高级SCSI编程接口）标准软件接口使驱动器和计算机内部安装的SCSI适配器进行通讯.缺点是价格过于昂贵. SCSI寻址机制和几个阶段 空闲阶段 仲裁阶段SCSI总线寻址方式，控制器-通道-SCSI ID -LUN ID来寻址.LUN（Logical Unit Number）对传统的SCSI总线来说意义不大，因为传统SCSI设备本身已经不可物理上再分了.因此设备的LUN ID为0.代表物理设备本身.对于带RAID功能的SCSI接口磁盘阵列设备来说，由于会产生的虚拟磁盘，所以只靠SCSI ID是不够的，这时候就要用LUN来扩充可寻址的范围. 选择阶段 磁盘控制器、驱动器 硬盘的接口包括接入到磁盘控制器上的物理接口以及定义的一套指令系统，即逻辑接口. 内部传输速率和外部传输速率 内部传输速率指刺头读写磁盘时的最高速率.这个速率不包括寻道以及等待扇区旋转到磁头下所用时间.通常每秒10000转的SCSI硬盘内部传输速率大概1000MB/s. 从外部接口传送给磁盘控制器时的传送速率就是硬盘的外部传输速率，10000转的SCSI硬盘实际外部传输速率只有80MB/s左右. 并行传输和串行传输 并行传输应用在长距离的连接上就无优点可言了.同时传输频率不能过高，因为电路高速震荡的时候，数据线之间会产生很大的干扰，所以并行传输的传输效率高但是速度慢. USB接口、IEEE1394接口以及COM接口都是串行传输的计算机外部接口. IO延迟、IOPS和传输带宽Throughput IO延迟是指控制器将IO指令发出后，知道IO完成的过程中所耗费的时间.业界又不成文的规定，IO延迟在20ms以内，对应用程序都是可以接受的.推算出IOPS=50. 控制器想存储设备发起指令不是一条一条顺序发送，而是一批一批的.也就是说，只要存储设备的肚量和消化能力足够，在IO比较少的时候，处理一条指令和多条指令的耗时几乎一样.控制器所发出批量指令的最大条数，有控制器上的Queue Depth决定. IOPS=（Queue Depth）/（IO Latency）对SCSI协议来说，完成一次连续LBA地址扇区的读写就算是一次IO，从底层看，每次想磁盘发送一个SCSI帧就是一次IO. 传输带宽 ： 具有高带宽规格的磁盘在传输大块连续数据时具有优势，而具有高IOPS的硬盘在传输小块不连续的数据时具有优势. 3 固态存储介质 结构对于一个16GB的FLASH，3214*8=34512个Cell逻辑上形成一个Page，每个Page中可以存放4KB的内容和218B的ECC校验数据，Page也是Flash芯片IO的最小单位.没128个Page组成一个Block，每2048个Block组成一个区域（Plane）.一整片Flash芯片由两个区域组成，一个区域存奇数序号的Blocks，一个存偶数序号的Blocks. 读写过程 对于小块随机IO，Flash会随着容量的增加而变得越来越低效.SSD的IO最小单位为1个Page. Flash写入之前必须Erase.Cell带负电荷电表示0，不带电表示1.Erase即是释放电子的过程.若写1，则不作任何操作；若写0，则该Cell充入电子，写0的过程叫programme. SSD的诟病与补救 拆东墙补西墙。为实现Wear Leveling，通过FTL映射实现IO的重定向. 定期清理体内垃圾 : Wiper TRIM支持 Delay Write减少对SSD的写 预留一部分空间不写 存储器的读写延迟总结","tags":[{"name":"存储","slug":"存储","permalink":"http://yoursite.com/tags/存储/"},{"name":"总线","slug":"总线","permalink":"http://yoursite.com/tags/总线/"}]}]